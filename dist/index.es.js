/**
 * Copyright Metrichor Ltd. (An Oxford Nanopore Technologies Company) 2020
 */

import{merge as e,flatten as t,remove as s,assign as i,filter as o,every as r,isFunction as n,defaults as a,isArray as l}from"lodash";import c from"fs-extra";import u,{homedir as h,EOL as p}from"os";import d from"path";import g from"aws-sdk";import f from"proxy-agent";import m from"axios";import w from"crypto";import{httpsOverHttps as y,httpsOverHttp as k}from"tunnel";import b from"readline";import S from"zlib";import $ from"graphql-tag";import{ApolloClient as v}from"apollo-client";import{InMemoryCache as E}from"apollo-cache-inmemory";import{ApolloLink as I,execute as _}from"apollo-link";import{createHttpLink as P}from"apollo-link-http";import{buildAxiosFetch as T}from"@lifeomic/axios-fetch";import j from"socket.io-client";import x from"sqlite";var N={name:"@metrichor/epi2me-api",version:"3.0.1459",license:"MPL-2.0",repository:"https://git.oxfordnanolabs.local/metrichor/api.git",description:"API for communicating with the EPI2ME website(s)",main:"dist/index.js",module:"dist/index.es.js",dependencies:{"@lifeomic/axios-fetch":"^1.4.1","@types/axios":"^0.14.0","@types/socket.io-client":"^1.4.32","apollo-cache-inmemory":"^1.6.3","apollo-client":"^2.6.4","apollo-link":"^1.2.13","apollo-link-context":"^1.0.19","apollo-link-http":"^1.5.16","aws-sdk":"^2.585.0",axios:"^0.19.0","core-js":"^3.4.8","fs-extra":"^8.1.0",graphql:"^14.5.8","graphql-tag":"^2.10.1",lodash:"4.17.15","proxy-agent":"^3.1.1",save:"^2.4.0","socket.io-client":"^2.3.0",sqlite:"^3.0.3",tunnel:"^0.0.6"},devDependencies:{"@babel/cli":"^7.7.5","@babel/core":"^7.7.5","@babel/plugin-proposal-object-rest-spread":"^7.7.4","@babel/preset-env":"^7.7.6","@babel/register":"^7.7.4","@types/bunyan":"^1.8.6","@types/rollup":"^0.54.0","@types/rollup-plugin-json":"^3.0.2","babel-eslint":"^10.0.3",bunyan:"^1.8.12",eslint:"^6.7.2","eslint-config-airbnb-base":"^14.0.0","eslint-config-defaults":"9.0.0","eslint-config-prettier":"^6.7.0","eslint-plugin-babel":"^5.3.0","eslint-plugin-import":"^2.19.1","eslint-plugin-prettier":"^3.1.1",husky:"^3.0.8","lint-staged":"^9.5.0",mocha:"6.2.2",nyc:"^14.1.1",prettier:"^1.18.2","prettier-eslint":"^9.0.0",rollup:"^1.27.9","rollup-plugin-analyzer":"^3.2.1","rollup-plugin-cpy":"^2.0.1","rollup-plugin-eslint":"^7.0.0","rollup-plugin-generate-package-json":"^3.1.3","rollup-plugin-json":"^4.0.0","rollup-plugin-license":"^0.13.0","rollup-plugin-terser":"^5.1.2",sinon:"7.5.0",tmp:"0.1.0","xunit-file":"*"},browserslist:[">0.2%","not dead","not ie <= 11","not op_mini all"],"lint-staged":{"*.{ts,tsx,js,jsx}":["npm run fix-ts --fix","git add --force"],"*.{json,md,graphql}":["prettier --write","git add --force"]},scripts:{"build:version":'jq ".version=\\"$(jq -r .version package.json | cut -d . -f 1-2).${PATCH:-$(date +%-H%M)}\\"" < package.json > package.json.tmp && mv package.json.tmp package.json',"lint-js":'eslint --ignore-path .eslintignore --ignore-pattern "!**/.*" .',"fix-js":"npm run lint-js --fix",lint:"npm run lint-js",deps:"npm ci","clean:dist":"rm -rf dist","clean:build":"rm -rf build && rm -rf dist/lib",clean:"npm run clean:build && npm run clean:dist",test:"npx mocha --recursive --require @babel/register test",cover:"npm install && npm run lint && npx nyc --reporter=html --reporter=text mocha --recursive --require @babel/register test",build:"npm ci && npm run build:dist","rollup:build":"npx rollup -c","rollup:watch":"npx rollup -cw","build:dist":"npm run build:version && npm run clean:dist && npm ci && npm run rollup:build"}};m.defaults.validateStatus=e=>e<=504;const O=function(){const t=(e,t)=>{e.headers||(e.headers={});let s=t;if(s||(s={}),!s.apikey)return;if(e.headers["X-EPI2ME-ApiKey"]=s.apikey,!s.apisecret)return;e.headers["X-EPI2ME-SignatureDate"]=(new Date).toISOString(),e.url.match(/^https:/)&&(e.url=e.url.replace(/:443/,"")),e.url.match(/^http:/)&&(e.url=e.url.replace(/:80/,""));const i=[e.url,Object.keys(e.headers).sort().filter(e=>e.match(/^x-epi2me/i)).map(t=>`${t}:${e.headers[t]}`).join("\n")].join("\n"),o=w.createHmac("sha1",s.apisecret).update(i).digest("hex");e.headers["X-EPI2ME-SignatureV0"]=o},s=async e=>{const t=e?e.data:null;if(!t)return Promise.reject(new Error("unexpected non-json response"));if(e&&e.status>=400){let s=`Network error ${e.status}`;return t.error&&(s=t.error),504===e.status&&(s="Please check your network connection and try again."),Promise.reject(new Error(s))}return t.error?Promise.reject(new Error(t.error)):Promise.resolve(t)};return{version:"3.0.1459",headers:(s,i)=>{const{log:o}=e({log:{debug:()=>{}}},i);let r=i;if(r||(r={}),s.headers=e({Accept:"application/json","Content-Type":"application/json","X-EPI2ME-Client":r.user_agent||"api","X-EPI2ME-Version":r.agent_version||O.version},s.headers,r.headers),"signing"in r&&!r.signing||t(s,r),r.proxy){const e=r.proxy.match(/https?:\/\/((\S+):(\S+)@)?(\S+):(\d+)/),t=e[2],i=e[3],n={host:e[4],port:e[5]};t&&i&&(n.proxyAuth=`${t}:${i}`),r.proxy.match(/^https/)?(o.debug("using HTTPS over HTTPS proxy",JSON.stringify(n)),s.httpsAgent=y({proxy:n})):(o.debug("using HTTPS over HTTP proxy",JSON.stringify(n)),s.httpsAgent=k({proxy:n})),s.proxy=!1}},get:async(t,i)=>{const{log:o}=e({log:{debug:()=>{}}},i);let r,n=i.url,a=t;i.skip_url_mangle?r=a:(a=`/${a}`,r=(n=n.replace(/\/+$/,""))+(a=a.replace(/\/+/g,"/")));const l={url:r,gzip:!0};let c;O.headers(l,i);try{o.debug(`GET ${l.url}`),c=await m.get(l.url,l)}catch(u){return Promise.reject(u)}return s(c,i)},post:async(t,i,o)=>{const{log:r}=e({log:{debug:()=>{}}},o);let n=o.url;const a={url:`${n=n.replace(/\/+$/,"")}/${t.replace(/\/+/g,"/")}`,gzip:!0,data:i,headers:{}};if(o.legacy_form){const t=[],s=e({json:JSON.stringify(i)},i);Object.keys(s).sort().forEach(e=>{t.push(`${e}=${escape(s[e])}`)}),a.data=t.join("&"),a.headers["Content-Type"]="application/x-www-form-urlencoded"}O.headers(a,o);const{data:l}=a;let c;delete a.data;try{r.debug(`POST ${a.url}`),c=await m.post(a.url,l,a)}catch(u){return Promise.reject(u)}return o.handler?o.handler(c):s(c,o)},put:async(t,i,o,r)=>{const{log:n}=e({log:{debug:()=>{}}},r);let a=r.url;const l={url:`${a=a.replace(/\/+$/,"")}/${t.replace(/\/+/g,"/")}/${i}`,gzip:!0,data:o,headers:{}};if(r.legacy_form){const t=[],s=e({json:JSON.stringify(o)},o);Object.keys(s).sort().forEach(e=>{t.push(`${e}=${escape(s[e])}`)}),l.data=t.join("&"),l.headers["Content-Type"]="application/x-www-form-urlencoded"}O.headers(l,r);const{data:c}=l;let u;delete l.data;try{n.debug(`PUT ${l.url}`),u=await m.put(l.url,c,l)}catch(h){return Promise.reject(h)}return s(u,r)}}}();O.pipe=async(e,t,s,i)=>{let o=s.url,r=`/${e}`;const n={url:(o=o.replace(/\/+$/,""))+(r=r.replace(/\/+/g,"/")),gzip:!0,headers:{"Accept-Encoding":"gzip",Accept:"application/gzip"}};return O.headers(n,s),s.proxy&&(n.proxy=s.proxy),i&&(n.onUploadProgress=i),n.responseType="stream",new Promise((e,s)=>{m.get(n.url,n).then(i=>{const o=c.createWriteStream(t);i.data.pipe(o),o.on("finish",()=>{e(t)}),o.on("error",e=>{s(new Error(`writer failed ${String(e)}`))})}).catch(e=>{s(e)})})};let F=0;O.getFileID=()=>`FILE_${F+=1}`,O.lsRecursive=async(e,s,i)=>{let o=e;const r=c.statSync(s);if(i){if(await i(s,r))return null}return r.isDirectory()?c.readdir(s).then(e=>e.map(e=>d.join(s,e))).then(e=>Promise.all(e.map(e=>O.lsRecursive(o,e,i)))).then(e=>t(e)):(r.isFile()&&o===s&&(o=d.dirname(s)),[{name:d.parse(s).base,path:s,relative:s.replace(o,""),size:r.size,id:O.getFileID()}])},O.loadInputFiles=async({inputFolder:e,outputFolder:t,filetype:i},o,r)=>{let n=i;n instanceof Array||(n=[n]),n=n.map(e=>e&&0!==e.indexOf(".")?`.${e}`:e);const a=await O.lsRecursive(e,e,async(e,s)=>{const i=d.basename(e),o=[new Promise((t,s)=>"downloads"===i||"skip"===i||"fail"===i||"fastq_fail"===i||"tmp"===i?s(new Error(`${e} failed basic filename`)):t("basic ok")),new Promise((o,r)=>{const a=n.length?new RegExp(`(?:${n.join("|")})$`):null;return e.split(d.sep).filter(e=>e.match(/^[.]/)).length||t&&i===d.basename(t)||a&&!e.match(a)&&s.isFile()?r(new Error(`${e} failed extended filename`)):o("extended ok")}),r?new Promise((t,s)=>{r(e).then(i=>i?s(new Error(`${e} failed extraFilter`)):t("extra ok"))}):Promise.resolve("extra skip")];return Promise.all(o).then(()=>null).catch(()=>"exclude")});return Promise.resolve(s(a,null))};var R=!1,M="https://epi2me.nanoporetech.com",A="EPI2ME API",C=!0,q={local:R,url:M,user_agent:A,region:"eu-west-1",sessionGrace:5,uploadTimeout:1200,downloadTimeout:1200,fileCheckInterval:5,downloadCheckInterval:3,stateCheckInterval:60,inFlightDelay:600,waitTimeSeconds:20,waitTokenError:30,transferPoolSize:3,downloadMode:"data+telemetry",filetype:[".fastq",".fq",".fastq.gz",".fq.gz"],signing:C};class W{constructor(e){this.options=i({agent_version:O.version,local:R,url:M,user_agent:A,signing:C},e),this.log=this.options.log}async list(e){try{const t=await O.get(e,this.options),s=e.match(/^[a-z_]+/i)[0];return Promise.resolve(t[`${s}s`])}catch(t){return this.log.error(`list error ${String(t)}`),Promise.reject(t)}}async read(e,t){try{const s=await O.get(`${e}/${t}`,this.options);return Promise.resolve(s)}catch(s){return this.log.error("read",s),Promise.reject(s)}}async user(){return this.options.local?{accounts:[{id_user_account:"none",number:"NONE",name:"None"}]}:O.get("user",this.options)}async status(){return O.get("status",this.options)}async jwt(){try{const t=e=>e.headers["x-epi2me-jwt"]?Promise.resolve(e.headers["x-epi2me-jwt"]):Promise.reject(new Error("failed to fetch JWT")),s=await O.post("authenticate",{},e({handler:t},this.options));return Promise.resolve(s)}catch(t){return Promise.reject(t)}}async instanceToken(t,s){return O.post("token",e(s,{id_workflow_instance:t}),i({},this.options,{legacy_form:!0}))}async installToken(e){return O.post("token/install",{id_workflow:e},i({},this.options,{legacy_form:!0}))}async attributes(){return this.list("attribute")}async workflows(){return this.list("workflow")}async amiImages(){if(this.options.local)throw new Error("amiImages unsupported in local mode");return this.list("ami_image")}async amiImage(e,t,s){let i,o,r,n;if(e&&t&&s instanceof Function?(i=e,o=t,r=s,n="update"):e&&t instanceof Object&&!(t instanceof Function)?(i=e,o=t,n="update"):e instanceof Object&&t instanceof Function?(o=e,r=t,n="create"):e instanceof Object&&!t?(o=e,n="create"):(n="read",i=e,r=t instanceof Function?t:null),this.options.local){const e=new Error("ami_image unsupported in local mode");return r?r(e):Promise.reject(e)}if("update"===n)try{const e=await O.put("ami_image",i,o,this.options);return r?r(null,e):Promise.resolve(e)}catch(a){return r?r(a):Promise.reject(a)}if("create"===n)try{const e=await O.post("ami_image",o,this.options);return r?r(null,e):Promise.resolve(e)}catch(a){return r?r(a):Promise.reject(a)}if(!i){const e=new Error("no id_ami_image specified");return r?r(e):Promise.reject(e)}try{const e=await this.read("ami_image",i);return r?r(null,e):Promise.resolve(e)}catch(a){return r?r(a):Promise.reject(a)}}async workflow(t,s,i){let r,n,a,l;if(t&&s&&i instanceof Function?(r=t,n=s,a=i,l="update"):t&&s instanceof Object&&!(s instanceof Function)?(r=t,n=s,l="update"):t instanceof Object&&s instanceof Function?(n=t,a=s,l="create"):t instanceof Object&&!s?(n=t,l="create"):(l="read",r=t,a=s instanceof Function?s:null),"update"===l)try{const e=await O.put("workflow",r,n,this.options);return a?a(null,e):Promise.resolve(e)}catch(p){return a?a(p):Promise.reject(p)}if("create"===l)try{const e=await O.post("workflow",n,this.options);return a?a(null,e):Promise.resolve(e)}catch(p){return a?a(p):Promise.reject(p)}if(!r){const e=new Error("no workflow id specified");return a?a(e):Promise.reject(e)}const c={};try{const t=await this.read("workflow",r);if(t.error)throw new Error(t.error);e(c,t)}catch(p){return this.log.error(`${r}: error fetching workflow ${String(p)}`),a?a(p):Promise.reject(p)}e(c,{params:{}});try{const t=await O.get(`workflow/config/${r}`,this.options);if(t.error)throw new Error(t.error);e(c,t)}catch(p){return this.log.error(`${r}: error fetching workflow config ${String(p)}`),a?a(p):Promise.reject(p)}const u=o(c.params,{widget:"ajax_dropdown"}),h=[...u.map((e,t)=>{const s=u[t];return new Promise((e,t)=>{const i=s.values.source.replace("{{EPI2ME_HOST}}","").replace(/&?apikey=\{\{EPI2ME_API_KEY\}\}/,"");O.get(i,this.options).then(t=>{const i=t[s.values.data_root];return i&&(s.values=i.map(e=>({label:e[s.values.items.label_key],value:e[s.values.items.value_key]}))),e()}).catch(e=>(this.log.error(`failed to fetch ${i}`),t(e)))})})];try{return await Promise.all(h),a?a(null,c):Promise.resolve(c)}catch(p){return this.log.error(`${r}: error fetching config and parameters ${String(p)}`),a?a(p):Promise.reject(p)}}async startWorkflow(e){return O.post("workflow_instance",e,i({},this.options,{legacy_form:!0}))}async stopWorkflow(e){return O.put("workflow_instance/stop",e,null,i({},this.options,{legacy_form:!0}))}async workflowInstances(e){if(e&&e.run_id)try{const t=(await O.get(`workflow_instance/wi?show=all&columns[0][name]=run_id;columns[0][searchable]=true;columns[0][search][regex]=true;columns[0][search][value]=${e.run_id};`,this.options)).data.map(e=>({id_workflow_instance:e.id_ins,id_workflow:e.id_flo,run_id:e.run_id,description:e.desc,rev:e.rev}));return Promise.resolve(t)}catch(t){return Promise.reject(t)}return this.list("workflow_instance")}async workflowInstance(e){return this.read("workflow_instance",e)}async workflowConfig(e){return O.get(`workflow/config/${e}`,this.options)}async register(e,t){return O.put("reg",e,{description:t||`${u.userInfo().username}@${u.hostname()}`},i({},this.options,{signing:!1}))}async datasets(e){let t=e;return t||(t={}),t.show||(t.show="mine"),this.list(`dataset?show=${t.show}`)}async dataset(e){return this.options.local?this.datasets().then(t=>t.find(t=>t.id_dataset===e)):this.read("dataset",e)}async fetchContent(e,t){const s=i({},this.options,{skip_url_mangle:!0,headers:{"Content-Type":""}});try{const i=await O.get(e,s);return t?t(null,i):Promise.resolve(i)}catch(o){return t?t(o):Promise.reject(o)}}}class D extends W{async workflows(e){if(!this.options.local)return super.workflows(e);const t=d.join(this.options.url,"workflows");let s;try{return s=(await c.readdir(t)).filter(e=>c.statSync(d.join(t,e)).isDirectory()).map(e=>d.join(t,e,"workflow.json")).map(e=>c.readJsonSync(e)),e?e(null,s):Promise.resolve(s)}catch(i){return this.log.warn(i),e?e(void 0):Promise.reject(void 0)}}async workflow(e,t,s){if(!this.options.local||!e||"object"===typeof e||s)return super.workflow(e,t,s);const i=d.join(this.options.url,"workflows"),o=d.join(i,e,"workflow.json");try{const e=await c.readJson(o);return s?s(null,e):Promise.resolve(e)}catch(r){return s?s(r):Promise.reject(r)}}async workflowInstances(e,t){if(!this.options.local)return super.workflowInstances(e,t);let s,i;if(!e||e instanceof Function||void 0!==t?(s=e,i=t):i=e,i){const e=new Error("querying of local instances unsupported in local mode");return s?s(e):Promise.reject(e)}const o=d.join(this.options.url,"instances");try{let e=await c.readdir(o);return e=(e=e.filter(e=>c.statSync(d.join(o,e)).isDirectory())).map(e=>{const t=d.join(o,e,"workflow.json");let s;try{s=c.readJsonSync(t)}catch(i){s={id_workflow:"-",description:"-",rev:"0.0"}}return s.id_workflow_instance=e,s.filename=t,s}),s?s(null,e):Promise.resolve(e)}catch(r){return s?s(r):Promise.reject(r)}}async datasets(e,t){if(!this.options.local)return super.datasets(e,t);let s,i;if(!e||e instanceof Function||void 0!==t?(s=e,i=t):i=e,i||(i={}),i.show||(i.show="mine"),"mine"!==i.show)return s(new Error("querying of local datasets unsupported in local mode"));const o=d.join(this.options.url,"datasets");try{let e=await c.readdir(o);e=e.filter(e=>c.statSync(d.join(o,e)).isDirectory());let t=0;return e=e.sort().map(e=>({is_reference_dataset:!0,summary:null,dataset_status:{status_label:"Active",status_value:"active"},size:0,prefix:e,id_workflow_instance:null,id_account:null,is_consented_human:null,data_fields:null,component_id:null,uuid:e,is_shared:!1,id_dataset:t+=1,id_user:null,last_modified:null,created:null,name:e,source:e,attributes:null})),s?s(null,e):Promise.resolve(e)}catch(r){return this.log.warn(r),s?s(null,[]):Promise.resolve([])}}async bundleWorkflow(e,t,s){return O.pipe(`workflow/bundle/${e}.tar.gz`,t,this.options,s)}}const Q={fastq:function(e){return new Promise((t,s)=>{let i,o=1,r={size:0};try{r=c.statSync(e)}catch(n){return void s(n)}c.createReadStream(e).on("data",e=>{i=-1,o-=1;do{i=e.indexOf(10,i+1),o+=1}while(-1!==i)}).on("end",()=>t({type:"fastq",bytes:r.size,reads:Math.floor(o/4)})).on("error",s)})},fasta:function(e){return new Promise((t,s)=>{let i,o=1,r={size:0};try{r=c.statSync(e)}catch(n){s(n)}c.createReadStream(e).on("data",e=>{i=-1,o-=1;do{i=e.indexOf(62,i+1),o+=1}while(-1!==i)}).on("end",()=>t({type:"fasta",bytes:r.size,sequences:Math.floor((1+o)/2)})).on("error",s)})},default:async function(e){return c.stat(e).then(e=>({type:"bytes",bytes:e.size}))}};function U(e){if("string"!==typeof e&&!(e instanceof String))return Promise.resolve({});let t=d.extname(e).toLowerCase().replace(/^[.]/,"");return"fq"===t?t="fastq":"fa"===t&&(t="fasta"),Q[t]||(t="default"),Q[t](e)}async function z(t,s,i,o,r,n){const{maxChunkBytes:a,maxChunkReads:l}=e({},s),u=d.dirname(t),h=d.basename(t),p=h.match(/^[^.]+/)[0],g=h.replace(p,""),f=d.join(u,p);if(!a&&!l)return i(t).then(()=>({source:t,split:!1,chunks:[t]}));const m=await c.stat(t);return a&&m.size<a?i(t).then(()=>({source:t,split:!1,chunks:[t]})):new Promise(s=>{let u,h,p=0,d=0,m="",w=0,y=0;const k={source:t,split:!0,chunks:[]};let S;const $=[new Promise(e=>{S=e})];b.createInterface({input:r(t)}).on("line",async e=>{m+=e,m+="\n",(d+=1)>=4&&(d=0,(async e=>{if(!w){u=`${f}_${p+=1}${g}`;const e=new Promise((e,t)=>{const s=u,r=()=>{i(s).then(()=>{e(s)}).catch(e=>{t(e)}).finally(()=>{c.unlink(s).catch(e=>{o.warn(`Error unlinking chunk ${s}: ${String(e)}`)})})};n?h=n(s,r):(h=c.createWriteStream(s)).on("close",r)});$.push(e)}w+=1,y+=e.length,h.write(e,()=>{}),(a&&y>=a||l&&w>=l)&&(w=0,y=0,h.end())})(m),m="")}).on("close",()=>{h.end(),S(),Promise.all($).then(t=>{t.shift(),s(e({chunks:t},k))})}).on("error",e=>{o.error(`Error chunking ${t}: ${String(e)}`)})})}async function L(e,t,s,i){return z(e,t,s,i,e=>c.createReadStream(e))}async function J(e,t,s,i){return z(e,t,s,i,e=>c.createReadStream(e).pipe(S.createGunzip()),(e,t)=>{const s=c.createWriteStream(e);s.on("close",t);const i=S.createGzip();return i.pipe(s),i})}const H=(e,t)=>{const s=["","K","M","G","T","P","E","Z"];let i=t||0,o=e||0;return o>=1e3?(o/=1e3,(i+=1)>=s.length?"???":H(o,i)):0===i?`${o}${s[i]}`:`${o.toFixed(1)}${s[i]}`},B=function(){const t=(e,t)=>{e.headers||(e.headers={});let s=t;if(s||(s={}),!s.apikey||!s.apisecret)return;e.headers["X-EPI2ME-APIKEY"]=s.apikey,e.headers["X-EPI2ME-SIGNATUREDATE"]=(new Date).toISOString();const i=[Object.keys(e.headers).sort().filter(e=>e.match(/^x-epi2me/i)).map(t=>`${t}:${e.headers[t]}`).join("\n"),e.body].join("\n"),o=w.createHmac("sha1",s.apisecret).update(i).digest("hex");e.headers["X-EPI2ME-SIGNATUREV0"]=o};return{version:"3.0.1459",setHeaders:(s,i)=>{const{log:o}=e({log:{debug:()=>{}}},i);let r=i;if(r||(r={}),s.headers=e({Accept:"application/json","Content-Type":"application/json","X-EPI2ME-CLIENT":r.user_agent||"api","X-EPI2ME-VERSION":r.agent_version||B.version},s.headers,r.headers),"signing"in r&&!r.signing||t(s,r),r.proxy){const e=r.proxy.match(/https?:\/\/((\S+):(\S+)@)?(\S+):(\d+)/),t=e[2],i=e[3],n={host:e[4],port:e[5]};t&&i&&(n.proxyAuth=`${t}:${i}`),r.proxy.match(/^https/)?(o.debug("using HTTPS over HTTPS proxy",JSON.stringify(n)),s.httpsAgent=y({proxy:n})):(o.debug("using HTTPS over HTTP proxy",JSON.stringify(n)),s.httpsAgent=k({proxy:n})),s.proxy=!1}}}}(),G=T(m),K=(e,t)=>{const{apikey:s,apisecret:i}=t.headers.keys;return delete t.headers.keys,B.setHeaders(t,{apikey:s,apisecret:i,signing:!0}),G(e,t)},X=new v({link:new I(e=>{const{apikey:t,apisecret:s,url:i}=e.getContext(),o=P({uri:`${i}/graphql`,fetch:K,headers:{keys:{apikey:t,apisecret:s}}});return _(o,e)}),cache:new E}),Y="\npage\npages\nhasNext\nhasPrevious\ntotalCount\n",V="\nidWorkflow\nname\ndescription\nsummary\n",Z="\nidWorkflowInstance\noutputqueue\nstartDate\n";class ee{constructor(e){this.options=i({agent_version:O.version,local:R,url:M,user_agent:A,signing:C},e),this.options.url=this.options.url.replace(/:\/\//,"://graphql."),this.log=this.options.log,this.client=X}createContext(t){const{apikey:s,apisecret:i,url:o}=this.options;return e({apikey:s,apisecret:i,url:o},t)}workflows(e={},t={}){const s=$`
      query allWorkflows($page: Int) {
        allWorkflows(page: $page) {
          ${Y}
          results {
            ${V}
          }
        }
      }
    `,i=this.createContext(e);return this.client.query({query:s,variables:t,context:i})}workflow(e){const t=$`
      query workflow($idWorkflow: ID!) {
        workflow(idWorkflow: $idWorkflow) {
          ${V}
        }
      }
    `;return this.client.query({query:t,variables:e})}workflowInstances(e){const t=$`
      query allWorkflowInstances($page: Int) {
        allWorkflowInstances(page: $page) {
          ${Y}
          results {
            ${Z}
          }
        }
      }
    `;return this.client.query({query:t,variables:e})}workflowInstance(e){const t=$`
      query workflowInstance($idWorkflowInstance: ID!) {
        workflowInstance(idWorkflowInstance: $idWorkflowInstance) {
          ${Z}
        }
      }
    `;return this.client.query({query:t,variables:e})}startWorkflow(e){const t=$`
      mutation startWorkflow(
        $idWorkflow: ID!
        $computeAccountId: Int!
        $storageAccountId: Int
        $isConsentedHuman: Int = 0
      ) {
        startWorkflowInstance(
          idWorkflow: $idWorkflow
          computeAccountId: $computeAccountId
          storageAccountId: $storageAccountId
          isConsentedHuman: $isConsentedHuman
        ) {
          bucket
          idUser
          idWorkflowInstance
          inputqueue
          outputqueue
          region
          keyId
          chain
        }
      }
    `;return this.client.mutate({mutation:t,variables:e})}async register(e,t,s){let i,o;t&&t instanceof Function?o=t:(i=t,o=s);try{const t=await O.post("apiaccess",{code:e,description:i||`${u.userInfo().username}@${u.hostname()}`},this.options);return o?o(null,t):Promise.resolve(t)}catch(r){return o?o(r):Promise.reject(r)}}}class te{constructor(t,s){this.debounces={},this.debounceWindow=e({debounceWindow:2e3},s).debounceWindow,this.log=e({log:{debug:()=>{}}},s).log,t.jwt().then(e=>{this.socket=j(s.url,{transportOptions:{polling:{extraHeaders:{Cookie:`x-epi2me-jwt=${e}`}}}}),this.socket.on("connect",()=>{this.log.debug("socket ready")})})}debounce(t,s){const i=e(t)._uuid;if(i){if(this.debounces[i])return;this.debounces[i]=1,setTimeout(()=>{delete this.debounces[i]},this.debounceWindow)}s&&s(t)}watch(e,t){if(!this.socket)return this.log.debug(`socket not ready. requeueing watch on ${e}`),void setTimeout(()=>{this.watch(e,t)},1e3);this.socket.on(e,e=>this.debounce(e,t))}emit(e,t){if(!this.socket)return this.log.debug(`socket not ready. requeueing emit on ${e}`),void setTimeout(()=>{this.emit(e,t)},1e3);this.log.debug(`socket emit ${e} ${JSON.stringify(t)}`),this.socket.emit(e,t)}}class se{constructor(t){let s;if((s="string"===typeof t||"object"===typeof t&&t.constructor===String?JSON.parse(t):t||{}).log){if(!r([s.log.info,s.log.warn,s.log.error,s.log.debug,s.log.json],n))throw new Error("expected log object to have error, debug, info, warn and json methods");this.log=s.log}else this.log={info:e=>{console.info(`[${(new Date).toISOString()}] INFO: ${e}`)},debug:e=>{console.debug(`[${(new Date).toISOString()}] DEBUG: ${e}`)},warn:e=>{console.warn(`[${(new Date).toISOString()}] WARN: ${e}`)},error:e=>{console.error(`[${(new Date).toISOString()}] ERROR: ${e}`)},json:e=>{console.log(JSON.stringify(e))}};this.stopped=!0,this.states={upload:{filesCount:0,success:{files:0,bytes:0,reads:0},types:{},niceTypes:"",progress:{bytes:0,total:0}},download:{progress:{},success:{files:0,reads:0,bytes:0},fail:0,types:{},niceTypes:""},warnings:[]},this.config={options:a(s,q),instance:{id_workflow_instance:s.id_workflow_instance,inputQueueName:null,outputQueueName:null,outputQueueURL:null,discoverQueueCache:{},bucket:null,bucketFolder:null,remote_addr:null,chain:null,key_id:null}},this.config.instance.awssettings={region:this.config.options.region},this.REST=new W(e({log:this.log},this.config.options)),this.graphQL=new ee(e({log:this.log},this.config.options)),this.timers={downloadCheckInterval:null,stateCheckInterval:null,fileCheckInterval:null,transferTimeouts:{},visibilityIntervals:{},summaryTelemetryInterval:null}}async socket(){return this.mySocket?this.mySocket:(this.mySocket=new te(this.REST,e({log:this.log},this.config.options)),this.mySocket)}async realtimeFeedback(e,t){(await this.socket()).emit(e,t)}async stopEverything(){this.stopped=!0,this.log.debug("stopping watchers"),["downloadCheckInterval","stateCheckInterval","fileCheckInterval","summaryTelemetryInterval"].forEach(e=>{this.timers[e]&&(this.log.debug(`clearing ${e} interval`),clearInterval(this.timers[e]),this.timers[e]=null)}),Object.keys(this.timers.transferTimeouts).forEach(e=>{this.log.debug(`clearing transferTimeout for ${e}`),clearTimeout(this.timers.transferTimeouts[e]),delete this.timers.transferTimeouts[e]}),Object.keys(this.timers.visibilityIntervals).forEach(e=>{this.log.debug(`clearing visibilityInterval for ${e}`),clearInterval(this.timers.visibilityIntervals[e]),delete this.timers.visibilityIntervals[e]}),this.downloadWorkerPool&&(this.log.debug("clearing downloadWorkerPool"),await Promise.all(Object.values(this.downloadWorkerPool)),this.downloadWorkerPool=null);const{id_workflow_instance:e}=this.config.instance;if(e){try{await this.REST.stopWorkflow(e)}catch(t){return this.log.error(`Error stopping instance: ${String(t)}`),Promise.reject(t)}this.log.info(`workflow instance ${e} stopped`)}return Promise.resolve()}reportProgress(){const{upload:e,download:t}=this.states;this.log.json({progress:{download:t,upload:e}})}storeState(e,t,s,i){const o=i||{};this.states[e]||(this.states[e]={}),this.states[e][t]||(this.states[e][t]={}),"incr"===s?Object.keys(o).forEach(s=>{this.states[e][t][s]=this.states[e][t][s]?this.states[e][t][s]+parseInt(o[s],10):parseInt(o[s],10)}):Object.keys(o).forEach(s=>{this.states[e][t][s]=this.states[e][t][s]?this.states[e][t][s]-parseInt(o[s],10):-parseInt(o[s],10)});try{this.states[e].success.niceReads=H(this.states[e].success.reads)}catch(n){this.states[e].success.niceReads=0}try{this.states[e].progress.niceSize=H(this.states[e].success.bytes+this.states[e].progress.bytes||0)}catch(n){this.states[e].progress.niceSize=0}try{this.states[e].success.niceSize=H(this.states[e].success.bytes)}catch(n){this.states[e].success.niceSize=0}this.states[e].niceTypes=Object.keys(this.states[e].types||{}).sort().map(t=>`${this.states[e].types[t]} ${t}`).join(", ");const r=Date.now();(!this.stateReportTime||r-this.stateReportTime>2e3)&&(this.stateReportTime=r,this.reportProgress())}uploadState(e,t,s){return this.storeState("upload",e,t,s)}downloadState(e,t,s){return this.storeState("download",e,t,s)}url(){return this.config.options.url}apikey(){return this.config.options.apikey}attr(e,t){if(!(e in this.config.options))throw new Error(`config object does not contain property ${e}`);return t?(this.config.options[e]=t,this):this.config.options[e]}stats(e){return this.states[e]}}se.version=O.version,se.REST=W,se.utils=O;class ie{constructor(t,s,i){const o=e({},s);this.options=o,this.log=i;const{idWorkflowInstance:r}=o;i.debug(`setting up ${t}/db.sqlite for ${r}`),this.db=c.mkdirp(t).then(()=>(this.log.debug(`opening ${t}/db.sqlite`),x.open(d.join(t,"db.sqlite"),{Promise:Promise}).then(async e=>{this.log.debug(`opened ${t}/db.sqlite`);try{return await Promise.all([e.run("CREATE TABLE IF NOT EXISTS meta (version CHAR(12) DEFAULT '' NOT NULL, idWorkflowInstance INTEGER UNSIGNED, inputFolder CHAR(255) default '')").then(()=>{e.run("INSERT INTO meta (version, idWorkflowInstance, inputFolder) VALUES(?, ?, ?)",N.version,r,o.inputFolder)}),e.run("CREATE TABLE IF NOT EXISTS uploads (filename CHAR(255) DEFAULT '' NOT NULL PRIMARY KEY)"),e.run("CREATE TABLE IF NOT EXISTS skips (filename CHAR(255) DEFAULT '' NOT NULL PRIMARY KEY)"),e.run("CREATE TABLE IF NOT EXISTS splits (filename CHAR(255) DEFAULT '' NOT NULL PRIMARY KEY, parent CHAR(255) DEFAULT '' NOT NULL, start DATETIME NOT NULL, end DATETIME)")]),Promise.resolve(e)}catch(s){return this.log.error(s),Promise.reject(s)}}))).catch(e=>{throw this.log.error(e),e})}async uploadFile(e){const t=await this.db,s=e.replace(new RegExp(`^${this.options.inputFolder}`),"");return t.run("INSERT INTO uploads VALUES(?)",s)}async skipFile(e){const t=await this.db,s=e.replace(new RegExp(`^${this.options.inputFolder}`),"");return t.run("INSERT INTO skips VALUES(?)",s)}async splitFile(e,t){const s=await this.db,i=e.replace(new RegExp(`^${this.options.inputFolder}`),""),o=t.replace(new RegExp(`^${this.options.inputFolder}`),"");return s.run("INSERT INTO splits VALUES(?, ?, CURRENT_TIMESTAMP, NULL)",i,o)}async splitDone(e){const t=await this.db,s=e.replace(new RegExp(`^${this.options.inputFolder}`),"");return t.run("UPDATE splits SET end=CURRENT_TIMESTAMP WHERE filename=?",s)}async splitClean(){return(await this.db).all("SELECT filename FROM splits WHERE end IS NULL").then(e=>{if(!e)return this.log.info("no split files to clean"),Promise.resolve();this.log.info(`cleaning ${e.length} split files`),this.log.debug(`going to clean: ${e.map(e=>e.filename).join(" ")}`);const t=e.map(e=>c.unlink(d.join(this.options.inputFolder,e.filename)).catch(()=>{}));return Promise.all(t)})}async seenUpload(e){const t=await this.db,i=e.replace(new RegExp(`^${this.options.inputFolder}`),"");return Promise.all([t.get("SELECT * FROM uploads u WHERE u.filename=? LIMIT 1",i),t.get("SELECT * FROM skips s WHERE s.filename=? LIMIT 1",i)]).then(e=>s(e,void 0).length)}}class oe{constructor(t,s){this.prefsFile=t||oe.profilePath(),this.allProfileData={},this.defaultEndpoint=process.env.METRICHOR||q.endpoint||q.url,this.raiseExceptions=s;try{this.allProfileData=e(c.readJSONSync(this.prefsFile),{profiles:{}}),this.allProfileData.endpoint&&(this.defaultEndpoint=this.allProfileData.endpoint)}catch(i){if(this.raiseExceptions)throw i}}static profilePath(){return d.join(h(),".epi2me.json")}profile(t,s){if(t&&s){e(this.allProfileData,{profiles:{[t]:s}});try{c.writeJSONSync(this.prefsFile,this.allProfileData)}catch(i){if(this.raiseExceptions)throw i}}return t?e({endpoint:this.defaultEndpoint},this.allProfileData.profiles[t]):{}}profiles(){return Object.keys(this.allProfileData.profiles||{})}}class re{static MakeQueryablePromise(e){if(e.isResolved)return e;let t=!0,s=!1,i=!1;const o=e.then(e=>(i=!0,t=!1,e)).catch(e=>{throw s=!0,t=!1,e});return o.dependsOn=e,o.isResolved=()=>i,o.isPending=()=>t,o.isRejected=()=>s,o}constructor(t){const s=e({bandwidth:1,interval:500},t);this.bandwidth=s.bandwidth,this.interval=s.interval,this.pipeline=[],this.running=[],this.completed=0,this.intervalId=null,"start"in s&&!s.start||this.start()}enqueue(e){this.pipeline.push(e)}start(){this.intervalId||(this.intervalId=setInterval(()=>{this.monitorInterval()},this.interval))}stop(){clearInterval(this.intervalId),delete this.intervalId}state(){return{queued:this.pipeline.length,running:this.running.length,completed:this.completed,state:this.intervalId?"running":"stopped"}}monitorInterval(){this.running.map((e,t)=>e.isPending()?null:t).filter(e=>e).reverse().forEach(e=>{this.running.splice(e,1),this.completed+=1});const e=this.bandwidth-this.running.length;for(let t=0;t<e;t+=1){const e=this.pipeline.shift();if(!e)return;this.running.push(re.MakeQueryablePromise(e()))}}}const ne=()=>{const e=process.env.APPDATA||("darwin"===process.platform?d.join(h(),"Library/Application Support"):h());return process.env.EPI2ME_HOME||d.join(e,"linux"===process.platform?".epi2me":"EPI2ME")};class ae extends se{constructor(t){super(t),this.REST=new D(e({},{log:this.log},this.config.options))}async session(e,t){let s=!1;if(e&&e.length&&(s=!0),!s){if(this.sessioning)return Promise.resolve();if(this.states.sts_expiration&&this.states.sts_expiration>Date.now())return Promise.resolve();this.sessioning=!0}return this.fetchInstanceToken(e,t).catch(e=>{throw this.log.error(`session error ${String(e)}`),e}).finally(()=>{s||(this.sessioning=!1)})}async fetchInstanceToken(t,s){if(!this.config.instance.id_workflow_instance)return Promise.reject(new Error("must specify id_workflow_instance"));this.log.debug("new instance token needed");try{const i=await this.REST.instanceToken(this.config.instance.id_workflow_instance,s);this.log.debug(`allocated new instance token expiring at ${i.expiration}`),this.states.sts_expiration=new Date(i.expiration).getTime()-60*this.config.options.sessionGrace;const o={};this.config.options.proxy&&e(o,{httpOptions:{agent:f(this.config.options.proxy,!0)}}),e(o,this.config.instance.awssettings,i),g.config.update(o),t&&t.forEach(e=>{try{e.config.update(o)}catch(t){this.log.warn(`failed to update config on ${String(e)}: ${String(t)}`)}})}catch(i){this.log.warn(`failed to fetch instance token: ${String(i)}`)}return Promise.resolve()}async sessionedS3(e){return await this.session(null,e),new g.S3({useAccelerateEndpoint:"on"===this.config.options.awsAcceleration})}async sessionedSQS(e){return await this.session(null,e),new g.SQS}async deleteMessage(e){try{const t=await this.discoverQueue(this.config.instance.outputQueueName);return(await this.sessionedSQS()).deleteMessage({QueueUrl:t,ReceiptHandle:e.ReceiptHandle}).promise()}catch(t){return this.log.error(`deleteMessage exception: ${String(t)}`),this.states.download.failure||(this.states.download.failure={}),this.states.download.failure[t]=this.states.download.failure[t]?this.states.download.failure[t]+1:1,Promise.reject(t)}}async discoverQueue(e){if(this.config.instance.discoverQueueCache[e])return Promise.resolve(this.config.instance.discoverQueueCache[e]);let t;this.log.debug(`discovering queue for ${e}`);try{const s=await this.sessionedSQS();t=await s.getQueueUrl({QueueName:e}).promise()}catch(s){return this.log.error(`Error: failed to find queue for ${e}: ${String(s)}`),Promise.reject(s)}return this.log.debug(`found queue ${t.QueueUrl}`),this.config.instance.discoverQueueCache[e]=t.QueueUrl,Promise.resolve(t.QueueUrl)}async queueLength(e){if(!e)return Promise.reject(new Error("no queueURL specified"));const t=e.match(/([\w\-_]+)$/)[0];this.log.debug(`querying queue length of ${t}`);try{const t=await this.sessionedSQS(),s=await t.getQueueAttributes({QueueUrl:e,AttributeNames:["ApproximateNumberOfMessages"]}).promise();if(s&&s.Attributes&&"ApproximateNumberOfMessages"in s.Attributes){let e=s.Attributes.ApproximateNumberOfMessages;return e=parseInt(e,10)||0,Promise.resolve(e)}return Promise.reject(new Error("unexpected response"))}catch(s){return this.log.error(`error in getQueueAttributes ${String(s)}`),Promise.reject(s)}}async autoStart(e,t){let s;this.stopped=!1;try{s=await this.REST.startWorkflow(e)}catch(i){const e=`Failed to start workflow: ${String(i)}`;return this.log.warn(e),t?t(e):Promise.reject(i)}return this.config.workflow=JSON.parse(JSON.stringify(e)),this.log.info(`instance ${JSON.stringify(s)}`),this.log.info(`workflow config ${JSON.stringify(this.config.workflow)}`),this.autoConfigure(s,t)}async autoJoin(e,t){let s;this.stopped=!1,this.config.instance.id_workflow_instance=e;try{s=await this.REST.workflowInstance(e)}catch(i){const e=`Failed to join workflow instance: ${String(i)}`;return this.log.warn(e),t?t(e):Promise.reject(i)}return"stopped"===s.state?(this.log.warn(`workflow ${e} is already stopped`),t?t("could not join workflow"):Promise.reject(new Error("could not join workflow"))):(this.config.workflow=this.config.workflow||{},this.log.debug(`instance ${JSON.stringify(s)}`),this.log.debug(`workflow config ${JSON.stringify(this.config.workflow)}`),this.autoConfigure(s,t))}async autoConfigure(e,t){if(["id_workflow_instance","id_workflow","remote_addr","key_id","bucket","user_defined","start_date","id_user"].forEach(t=>{this.config.instance[t]=e[t]}),this.config.instance.inputQueueName=e.inputqueue,this.config.instance.outputQueueName=e.outputqueue,this.config.instance.awssettings.region=e.region||this.config.options.region,this.config.instance.bucketFolder=`${e.outputqueue}/${e.id_user}/${e.id_workflow_instance}`,this.config.instance.summaryTelemetry=e.telemetry,e.chain)if("object"===typeof e.chain)this.config.instance.chain=e.chain;else try{this.config.instance.chain=JSON.parse(e.chain)}catch(a){throw new Error(`exception parsing chain JSON ${String(a)}`)}if(!this.config.options.inputFolder)throw new Error("must set inputFolder");if(!this.config.options.outputFolder)throw new Error("must set outputFolder");if(!this.config.instance.bucketFolder)throw new Error("bucketFolder must be set");if(!this.config.instance.inputQueueName)throw new Error("inputQueueName must be set");if(!this.config.instance.outputQueueName)throw new Error("outputQueueName must be set");c.mkdirpSync(this.config.options.outputFolder);const s=d.join(ne(),"instances"),i=d.join(s,this.config.instance.id_workflow_instance);this.db=new ie(i,{idWorkflowInstance:this.config.instance.id_workflow_instance,inputFolder:this.config.options.inputFolder},this.log);const o=this.config.instance.id_workflow_instance?`telemetry-${this.config.instance.id_workflow_instance}.log`:"telemetry.log",r=d.join(this.config.options.outputFolder,"epi2me-logs"),n=d.join(r,o);return c.mkdirp(r,e=>{if(e&&!String(e).match(/EEXIST/))this.log.error(`error opening telemetry log stream: mkdirpException:${String(e)}`);else try{this.telemetryLogStream=c.createWriteStream(n,{flags:"a"}),this.log.info(`logging telemetry to ${n}`)}catch(t){this.log.error(`error opening telemetry log stream: ${String(t)}`)}}),t&&t(null,this.config.instance),this.timers.summaryTelemetryInterval=setInterval(()=>{this.stopped?clearInterval(this.timers.summaryTelemetryInterval):this.fetchTelemetry()},1e4*this.config.options.downloadCheckInterval),this.timers.downloadCheckInterval=setInterval(()=>{this.stopped?clearInterval(this.timers.downloadCheckInterval):this.checkForDownloads()},1e3*this.config.options.downloadCheckInterval),this.timers.stateCheckInterval=setInterval(async()=>{if(this.stopped)clearInterval(this.timers.stateCheckInterval);else try{const t=await this.REST.workflowInstance(this.config.instance.id_workflow_instance);if("stopped"===t.state){this.log.warn(`instance was stopped remotely at ${t.stop_date}. shutting down the workflow.`);try{const e=await this.stopEverything();"function"===typeof e.config.options.remoteShutdownCb&&e.config.options.remoteShutdownCb(`instance was stopped remotely at ${t.stop_date}`)}catch(e){this.log.error(`Error whilst stopping: ${String(e)}`)}}}catch(t){this.log.warn(`failed to check instance state: ${t&&t.error?t.error:t}`)}},1e3*this.config.options.stateCheckInterval),await this.session(),this.reportProgress(),this.loadUploadFiles(),this.timers.fileCheckInterval=setInterval(this.loadUploadFiles.bind(this),1e3*this.config.options.fileCheckInterval),Promise.resolve(e)}async stopEverything(){return await super.stopEverything(),this.log.debug("clearing split files"),this.db?this.db.splitClean():Promise.resolve()}async checkForDownloads(){if(this.checkForDownloadsRunning)return Promise.resolve();this.checkForDownloadsRunning=!0,this.log.debug("checkForDownloads checking for downloads");try{const e=await this.discoverQueue(this.config.instance.outputQueueName),t=await this.queueLength(e);t?(this.log.debug(`downloads available: ${t}`),await this.downloadAvailable()):this.log.debug("no downloads available")}catch(e){this.log.warn(`checkForDownloads error ${String(e)}`),this.states.download.failure||(this.states.download.failure={}),this.states.download.failure[e]=this.states.download.failure[e]?this.states.download.failure[e]+1:1}return this.checkForDownloadsRunning=!1,Promise.resolve()}async downloadAvailable(){const e=Object.keys(this.downloadWorkerPool||{}).length;if(e>=this.config.options.transferPoolSize)return this.log.debug(`${e} downloads already queued`),Promise.resolve();let t;try{const s=await this.discoverQueue(this.config.instance.outputQueueName);this.log.debug("fetching messages");const i=await this.sessionedSQS();t=await i.receiveMessage({AttributeNames:["All"],QueueUrl:s,VisibilityTimeout:this.config.options.inFlightDelay,MaxNumberOfMessages:this.config.options.transferPoolSize-e,WaitTimeSeconds:this.config.options.waitTimeSeconds}).promise()}catch(s){return this.log.error(`receiveMessage exception: ${String(s)}`),this.states.download.failure[s]=this.states.download.failure[s]?this.states.download.failure[s]+1:1,Promise.reject(s)}return this.receiveMessages(t)}async loadUploadFiles(){if(this.dirScanInProgress)return Promise.resolve();this.dirScanInProgress=!0,this.log.debug("upload: started directory scan");try{const e=e=>this.db.seenUpload(e),t=await O.loadInputFiles(this.config.options,this.log,e);let s=0;const i=()=>new Promise(e=>{if(this.stopped)return t.length=0,this.log.debug("upload: skipping, stopped"),void e();if(s>this.config.options.transferPoolSize)return void setTimeout(e,1e3);const i=t.splice(0,this.config.options.transferPoolSize-s);s+=i.length,this.enqueueUploadFiles(i).then().catch(e=>{this.log.error(`upload: exception in enqueueUploadFiles: ${String(e)}`)}).finally(()=>{s-=i.length,e()})});for(;t.length;)await i()}catch(e){this.log.error(`upload: exception in loadInputFiles: ${String(e)}`)}return this.dirScanInProgress=!1,this.log.debug("upload: finished directory scan"),Promise.resolve()}async enqueueUploadFiles(e){let t=0,s=0,i=0,o=0,r={};if(!l(e)||!e.length)return Promise.resolve();if(this.log.info(`enqueueUploadFiles ${e.length} files: ${e.map(e=>e.path).join(" ")}.`),"workflow"in this.config)if("workflow_attributes"in this.config.workflow)r=this.config.workflow.workflow_attributes;else if("attributes"in this.config.workflow){let{attributes:e}=this.config.workflow;if(e||(e={}),["max_size","max_files","split_size","split_reads"].forEach(t=>{`epi2me:${t}`in e&&(r[t]=parseInt(e[`epi2me:${t}`],10))}),"epi2me:category"in e){e["epi2me:category"].includes("storage")&&(r.requires_storage=!0)}}if(this.log.info(`enqueueUploadFiles settings ${JSON.stringify(r)}`),"requires_storage"in r&&r.requires_storage&&!("storage_account"in this.config.workflow)){const e={msg:"ERROR: Workflow requires storage enabled. Please provide a valid storage account [ --storage ].",type:"WARNING_STORAGE_ENABLED"};return this.log.error(e.msg),this.states.warnings.push(e),Promise.resolve()}if("split_size"in r&&(i=parseInt(r.split_size,10),this.log.info(`enqueueUploadFiles splitting supported files at ${i} bytes`)),"split_reads"in r&&(o=parseInt(r.split_reads,10),this.log.info(`enqueueUploadFiles splitting supported files at ${o} reads`)),"max_size"in r&&(s=parseInt(r.max_size,10),this.log.info(`enqueueUploadFiles restricting file size to ${s}`)),"max_files"in r&&(t=parseInt(r.max_files,10),this.log.info(`enqueueUploadFiles restricting file count to ${t}`),e.length>t)){const s={msg:`ERROR: ${e.length} files found. Workflow can only accept ${t}. Please move the extra files away.`,type:"WARNING_FILE_TOO_MANY"};return this.log.error(s.msg),this.states.warnings.push(s),Promise.resolve()}this.states.upload.filesCount+=e.length;const n=e.map(async e=>{const r=e;if(t&&this.states.upload.filesCount>t){const e=`Maximum ${t} file(s) already uploaded. Marking ${r.relative} as skipped.`,s={msg:e,type:"WARNING_FILE_TOO_MANY"};this.log.error(e),this.states.warnings.push(s),this.states.upload.filesCount-=1,r.skip="SKIP_TOO_MANY"}else if(0===r.size){const e=`The file "${r.relative}" is empty. It will be skipped.`,t={msg:e,type:"WARNING_FILE_EMPTY"};r.skip="SKIP_EMPTY",this.states.upload.filesCount-=1,this.log.error(e),this.states.warnings.push(t)}else{if(r.path&&r.path.match(/\.(?:fastq|fq)(?:\.gz)?$/)&&(i&&r.size>i||o)){const e=`${r.relative}${r.size>i?" is too big and":""} is going to be split`;this.log.warn(e);const t={msg:e,type:"WARNING_FILE_SPLIT"};this.states.warnings.push(t);const s=i?{maxChunkBytes:i}:{maxChunkReads:o},a=r.path.match(/\.gz$/)?J:L,l=O.getFileID(),c=new re({bandwidth:this.config.options.transferPoolSize});let u=0;const h=async e=>(this.log.debug(`chunkHandler for ${e}`),await this.db.splitFile(e,r.path),this.stopped?(c.stop(),this.log.info(`stopped, so skipping ${e}`),Promise.reject(new Error("stopped"))):(u+=1,U(e).then(t=>({name:d.basename(e),path:e,relative:e.replace(this.config.options.inputFolder,""),id:`${l}_${u}`,stats:t,size:t.bytes})).then(async e=>{const t=new Promise(t=>{c.enqueue(()=>(this.log.info(`chunk upload starting ${e.id} ${e.path}`),this.stopped?(this.log.info(`chunk upload skipped (stopped) ${e.id} ${e.path}`),c.stop(),t(),Promise.resolve()):this.uploadJob(e).then(()=>this.db.splitDone(e.path)).catch(t=>{this.log.error(`chunk upload failed ${e.id} ${e.path}: ${String(t)}`)}).finally(t)))});await t})));try{await a(r.path,s,h,this.log),c.stop()}catch(n){if(c.stop(),"Error: stopped"===String(n))return Promise.resolve();throw n}return this.db.uploadFile(r.path)}if(s&&r.size>s){const e=`The file "${r.relative}" is bigger than the maximum size limit (${H(s)}B). It will be skipped.`,t={msg:e,type:"WARNING_FILE_TOO_BIG"};r.skip="SKIP_TOO_BIG",this.states.upload.filesCount-=1,this.log.error(e),this.states.warnings.push(t)}else try{r.stats=await U(r.path)}catch(a){this.log.error(`failed to stat ${r.path}: ${String(a)}`)}}return this.uploadJob(r)});try{return await Promise.all(n),this.log.info(`upload: inputBatchQueue (${n.length} jobs) complete`),this.loadUploadFiles()}catch(a){return this.log.error(`upload: enqueueUploadFiles exception ${String(a)}`),Promise.reject(a)}}async uploadJob(t){if("skip"in t)return this.db.skipFile(t.path);let s,i;try{this.log.info(`upload: ${t.id} starting`),s=await this.uploadHandler(t),this.log.info(`upload: ${s.id} uploaded and notified`)}catch(o){i=o,this.log.error(`upload: ${t.id} done, but failed: ${String(i)}`)}if(s||(s={}),i)this.log.error(`uploadJob ${i}`),this.states.upload.failure||(this.states.upload.failure={}),this.states.upload.failure[i]=this.states.upload.failure[i]?this.states.upload.failure[i]+1:1;else if(this.uploadState("success","incr",e({files:1},s.stats)),s.name){const e=d.extname(s.name);this.uploadState("types","incr",{[e]:1})}return Promise.resolve()}async receiveMessages(e){return e&&e.Messages&&e.Messages.length?(this.downloadWorkerPool||(this.downloadWorkerPool={}),e.Messages.forEach(e=>{this.downloadWorkerPool[e.MessageId]=1;const t=setTimeout(()=>{throw this.log.error(`this.downloadWorkerPool timeoutHandle. Clearing queue slot for message: ${e.MessageId}`),new Error("download timed out")},1e3*(60+this.config.options.downloadTimeout));this.processMessage(e).catch(e=>{this.log.error(`processMessage ${String(e)}`)}).finally(()=>{clearTimeout(t),e&&delete this.downloadWorkerPool[e.MessageId]})}),this.log.info(`downloader queued ${e.Messages.length} messages for processing`),Promise.resolve()):(this.log.info("complete (empty)"),Promise.resolve())}async processMessage(t){let s,i;if(!t)return this.log.debug("download.processMessage: empty message"),Promise.resolve();"Attributes"in t&&"ApproximateReceiveCount"in t.Attributes&&this.log.debug(`download.processMessage: ${t.MessageId} / ${t.Attributes.ApproximateReceiveCount}`);try{s=JSON.parse(t.Body)}catch(a){this.log.error(`error parsing JSON message.Body from message: ${JSON.stringify(t)} ${String(a)}`);try{await this.deleteMessage(t)}catch(l){this.log.error(`Exception deleting message: ${String(l)}`)}return Promise.resolve()}if(s.telemetry){const{telemetry:e}=s;if(e.tm_path)try{this.log.debug(`download.processMessage: ${t.MessageId} fetching telemetry`);const i=await this.sessionedS3(),o=await i.getObject({Bucket:s.bucket,Key:e.tm_path}).promise();this.log.info(`download.processMessage: ${t.MessageId} fetched telemetry`),e.batch=o.Body.toString("utf-8").split("\n").filter(e=>e&&e.length>0).map(e=>{try{return JSON.parse(e)}catch(l){return this.log.error(`Telemetry Batch JSON Parse error: ${String(l)}`),e}})}catch(u){this.log.error(`Could not fetch telemetry JSON: ${String(u)}`)}try{this.telemetryLogStream.write(JSON.stringify(e)+p)}catch(h){this.log.error(`error writing telemetry: ${h}`)}this.config.options.telemetryCb&&this.config.options.telemetryCb(e)}if(!s.path)return this.log.warn("nothing to download"),Promise.resolve();const o=s.path.match(/[\w\W]*\/([\w\W]*?)$/),r=o?o[1]:"";if(i=this.config.options.outputFolder,s.telemetry&&s.telemetry.hints&&s.telemetry.hints.folder){this.log.debug(`using folder hint ${s.telemetry.hints.folder}`);const e=s.telemetry.hints.folder.split("/").map(e=>e.toUpperCase());i=d.join.apply(null,[i,...e])}c.mkdirpSync(i);const n=d.join(i,r);if("data+telemetry"===this.config.options.downloadMode){const e=[""];let i=this.config&&this.config.workflow&&this.config.workflow.settings&&this.config.workflow.settings.output_format?this.config.workflow.settings.output_format:[];("string"===typeof i||i instanceof String)&&(i=i.trim().split(/[\s,]+/));try{e.push(...i)}catch(l){this.log.error(`Failed to work out workflow file suffixes: ${String(l)}`)}try{const i=e.map(e=>{const i=s.path+e,o=n+e;return this.log.debug(`download.processMessage: ${t.MessageId} downloading ${i} to ${o}`),new Promise((r,n)=>{this.initiateDownloadStream({bucket:s.bucket,path:i},t,o).then(r).catch(t=>{this.log.error(`Caught exception waiting for initiateDownloadStream: ${String(t)}`),e?n(t):r()})})});await Promise.all(i)}catch(l){this.log.error(`Exception fetching file batch: ${String(l)}`)}try{const e=!(!s.telemetry||!s.telemetry.json)&&s.telemetry.json.exit_status;e&&this.config.options.dataCb&&this.config.options.dataCb(n,e)}catch(u){this.log.warn(`failed to fire data callback: ${u}`)}}else{const e=s.telemetry.batch_summary&&s.telemetry.batch_summary.reads_num?s.telemetry.batch_summary.reads_num:1;this.downloadState("success","incr",{files:1,reads:e})}try{await this.deleteMessage(t)}catch(l){this.log.error(`Exception deleting message: ${String(l)}`)}return this.realtimeFeedback("workflow_instance:state",{type:"stop",id_workflow_instance:this.config.instance.id_workflow_instance,id_workflow:this.config.instance.id_workflow,component_id:"0",message_id:e(t).MessageId,id_user:this.config.instance.id_user}).catch(e=>{this.log.warn(`realtimeFeedback failed: ${String(e)}`)}),Promise.resolve()}async initiateDownloadStream(t,s,i){return new Promise(async(o,r)=>{let n,a,l;try{n=await this.sessionedS3()}catch(p){r(p)}const u=e=>{if(this.log.error(`Error during stream of bucket=${t.bucket} path=${t.path} to file=${i} ${String(e)}`),clearTimeout(this.timers.transferTimeouts[i]),delete this.timers.transferTimeouts[i],!a.networkStreamError)try{a.networkStreamError=1,a.close(),c.remove(i).then(()=>{this.log.warn(`removed failed download ${i}`)}).catch(e=>{this.log.warn(`failed to remove ${i}. unlinkException: ${String(e)}`)}),l.destroy&&(this.log.error(`destroying read stream for ${i}`),l.destroy())}catch(p){this.log.error(`error handling stream error: ${String(p)}`)}};try{const e={Bucket:t.bucket,Key:t.path};a=c.createWriteStream(i);const s=n.getObject(e);s.on("httpHeaders",(e,t)=>{this.downloadState("progress","incr",{total:parseInt(t["content-length"],10)})}),l=s.createReadStream()}catch(g){return this.log.error(`getObject/createReadStream exception: ${String(g)}`),void r(g)}l.on("error",u),a.on("finish",async()=>{if(!a.networkStreamError){this.log.debug(`downloaded ${i}`);try{const t=d.extname(i),s=await U(i);this.downloadState("success","incr",e({files:1},s)),this.downloadState("types","incr",{[t]:1}),this.downloadState("progress","decr",{total:s.bytes,bytes:s.bytes})}catch(t){this.log.warn(`failed to stat ${i}: ${String(t)}`)}this.reportProgress()}}),a.on("close",e=>{this.log.debug(`closing writeStream ${i}`),e&&this.log.error(`error closing write stream ${e}`),clearInterval(this.timers.visibilityIntervals[i]),delete this.timers.visibilityIntervals[i],clearTimeout(this.timers.transferTimeouts[i]),delete this.timers.transferTimeouts[i],setTimeout(this.checkForDownloads.bind(this)),this.log.info(`download.initiateDownloadStream: ${s.MessageId} downloaded ${t.path} to ${i}`),o()}),a.on("error",u);const h=()=>{u(new Error("transfer timed out"))};this.timers.transferTimeouts[i]=setTimeout(h,1e3*this.config.options.downloadTimeout);this.timers.visibilityIntervals[i]=setInterval(async()=>{this.stopped&&(clearInterval(this.timers.visibilityIntervals[i]),delete this.timers.visibilityIntervals[i]);const e=this.config.instance.outputQueueURL,t=s.ReceiptHandle;this.log.debug({message_id:s.MessageId},"updateVisibility");try{await this.sqs.changeMessageVisibility({QueueUrl:e,ReceiptHandle:t,VisibilityTimeout:this.config.options.inFlightDelay}).promise()}catch(o){this.log.error({message_id:s.MessageId,queue:e,error:o},"Error setting visibility"),clearInterval(this.timers.visibilityIntervals[i])}},900*this.config.options.inFlightDelay),l.on("data",e=>{clearTimeout(this.timers.transferTimeouts[i]),this.timers.transferTimeouts[i]=setTimeout(h,1e3*this.config.options.downloadTimeout),this.downloadState("progress","incr",{bytes:e.length})}).pipe(a)})}async uploadHandler(e){const t=await this.sessionedS3();let s;const i=e.relative.replace(/^[\\/]+/,"").replace(/\\/g,"/").replace(/\//g,"_"),o=[this.config.instance.bucketFolder,"component-0",i,i].join("/").replace(/\/+/g,"/");let r;return new Promise((i,n)=>{const a=()=>{s&&!s.closed&&s.close(),n(new Error(`${e.name} timed out`))};r=setTimeout(a,1e3*(this.config.options.uploadTimeout+5));try{s=c.createReadStream(e.path)}catch(l){return clearTimeout(r),void n(l)}s.on("error",e=>{s.close();let t="error in upload readstream";e&&e.message&&(t+=`: ${e.message}`),clearTimeout(r),n(new Error(t))}),s.on("open",()=>{const l={Bucket:this.config.instance.bucket,Key:o,Body:s};this.config.instance.key_id&&(l.SSEKMSKeyId=this.config.instance.key_id,l.ServerSideEncryption="aws:kms"),e.size&&(l["Content-Length"]=e.size),this.uploadState("progress","incr",{total:e.size});let c=0;const u=t.upload(l,{partSize:10485760,queueSize:1});u.on("httpUploadProgress",async e=>{if(this.stopped)n(new Error("stopped"));else{this.uploadState("progress","incr",{bytes:e.loaded-c}),c=e.loaded,clearTimeout(r),r=setTimeout(a,1e3*(this.config.options.uploadTimeout+5));try{await this.session([u.service])}catch(t){this.log.warn(`Error refreshing token: ${String(t)}`)}}}),u.promise().then(()=>{this.log.info(`${e.id} S3 upload complete`),s.close(),clearTimeout(r),this.uploadComplete(o,e).then(()=>{i(e)}).catch(e=>{n(e)}).finally(()=>{this.uploadState("progress","decr",{total:e.size,bytes:e.size})})}).catch(t=>{this.log.warn(`${e.id} uploadStreamError ${t}`),n(t)})})})}async uploadComplete(t,s){this.log.info(`${s.id} uploaded to S3: ${t}`);const i={bucket:this.config.instance.bucket,outputQueue:this.config.instance.outputQueueName,remote_addr:this.config.instance.remote_addr,user_defined:this.config.instance.user_defined||null,apikey:this.config.options.apikey,id_workflow_instance:this.config.instance.id_workflow_instance,id_master:this.config.instance.id_workflow,utc:(new Date).toISOString(),path:t,prefix:t.substring(0,t.lastIndexOf("/"))};if(this.config.instance.chain)try{i.components=JSON.parse(JSON.stringify(this.config.instance.chain.components)),i.targetComponentId=this.config.instance.chain.targetComponentId}catch(r){return this.log.error(`${s.id} exception parsing components JSON ${String(r)}`),Promise.reject(r)}if(this.config.instance.key_id&&(i.key_id=this.config.instance.key_id),this.config.options.agent_address)try{i.agent_address=JSON.parse(this.config.options.agent_address)}catch(n){this.log.error(`${s.id} Could not parse agent_address ${String(n)}`)}i.components&&Object.keys(i.components).forEach(e=>{"uploadMessageQueue"===i.components[e].inputQueueName&&(i.components[e].inputQueueName=this.uploadMessageQueue),"downloadMessageQueue"===i.components[e].inputQueueName&&(i.components[e].inputQueueName=this.downloadMessageQueue)});let o={};try{const e=await this.discoverQueue(this.config.instance.inputQueueName),t=await this.sessionedSQS();this.log.info(`${s.id} sending SQS message to input queue`),o=await t.sendMessage({QueueUrl:e,MessageBody:JSON.stringify(i)}).promise()}catch(a){return this.log.error(`${s.id} exception sending SQS message: ${String(a)}`),Promise.reject(a)}return this.realtimeFeedback("workflow_instance:state",{type:"start",id_workflow_instance:this.config.instance.id_workflow_instance,id_workflow:this.config.instance.id_workflow,component_id:"0",message_id:e(o).MessageId,id_user:this.config.instance.id_user}).catch(e=>{this.log.warn(`realtimeFeedback failed: ${String(e)}`)}),this.log.info(`${s.id} SQS message sent. Mark as uploaded`),this.db.uploadFile(s.path)}async fetchTelemetry(){if(!this.config||!this.config.instance||!this.config.instance.summaryTelemetry)return Promise.resolve();const e=d.join(ne(),"instances"),t=d.join(e,this.config.instance.id_workflow_instance),s=[];Object.keys(this.config.instance.summaryTelemetry).forEach(e=>{const i=this.config.instance.summaryTelemetry[e]||{},o=i[Object.keys(i)[0]];if(!o)return;const r=d.join(t,`${e}.json`);s.push(this.REST.fetchContent(o).then(e=>{c.writeJSONSync(r,e),this.log.debug(`fetched telemetry summary ${r}`)}).catch(e=>{this.log.debug(`Error fetching telemetry: ${String(e)}`)}))});let i=0;try{await Promise.all(s)}catch(o){i+=1}return i&&this.log.warn("summary telemetry incomplete"),Promise.resolve()}}ae.version=O.version,ae.REST=D,ae.utils=O,ae.EPI2ME_HOME=ne(),ae.Profile=oe;export default ae;
