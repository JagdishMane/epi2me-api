/**
 * Copyright Metrichor Ltd. (An Oxford Nanopore Technologies Company) 2020
 */

"use strict";function e(e){return e&&"object"===typeof e&&"default"in e?e.default:e}var t=e(require("aws-sdk")),s=e(require("fs-extra")),i=require("lodash"),o=require("os"),r=e(o),n=e(require("path")),a=e(require("sqlite")),l=e(require("axios")),c=e(require("crypto")),h=require("tunnel"),u=require("rxjs"),d=e(require("graphql-tag")),p=require("apollo-cache-inmemory"),g=e(require("apollo-client")),f=require("apollo-link"),m=require("apollo-link-http"),w=require("@lifeomic/axios-fetch"),y=e(require("socket.io-client")),k=e(require("recursive-readdir-async")),S=e(require("proxy-agent")),$=e(require("readline")),b=e(require("zlib")),_="3.0.1630";l.defaults.validateStatus=e=>e<=504;const v=function(){const e=(e,t)=>{e.headers||(e.headers={});let s=t;if(s||(s={}),!s.apikey)return;if(e.headers["X-EPI2ME-ApiKey"]=s.apikey,!s.apisecret)return;e.headers["X-EPI2ME-SignatureDate"]=(new Date).toISOString(),e.url.match(/^https:/)&&(e.url=e.url.replace(/:443/,"")),e.url.match(/^http:/)&&(e.url=e.url.replace(/:80/,""));const i=[e.url,Object.keys(e.headers).sort().filter(e=>e.match(/^x-epi2me/i)).map(t=>`${t}:${e.headers[t]}`).join("\n")].join("\n"),o=c.createHmac("sha1",s.apisecret).update(i).digest("hex");e.headers["X-EPI2ME-SignatureV0"]=o},t=async e=>{const t=e?e.data:null;if(!t)return Promise.reject(new Error("unexpected non-json response"));if(e&&e.status>=400){let s=`Network error ${e.status}`;return t.error&&(s=t.error),504===e.status&&(s="Please check your network connection and try again."),Promise.reject(new Error(s))}return t.error?Promise.reject(new Error(t.error)):Promise.resolve(t)};return{version:"3.0.1630",headers:(t,s)=>{const{log:o}=i.merge({log:{debug:()=>{}}},s);let r=s;if(r||(r={}),t.headers=i.merge({Accept:"application/json","Content-Type":"application/json","X-EPI2ME-Client":r.user_agent||"api","X-EPI2ME-Version":r.agent_version||v.version},t.headers,r.headers),"signing"in r&&!r.signing||e(t,r),r.proxy){const e=r.proxy.match(/https?:\/\/((\S+):(\S+)@)?(\S+):(\d+)/),s=e[2],i=e[3],n={host:e[4],port:e[5]};s&&i&&(n.proxyAuth=`${s}:${i}`),r.proxy.match(/^https/)?(o.debug("using HTTPS over HTTPS proxy",JSON.stringify(n)),t.httpsAgent=h.httpsOverHttps({proxy:n})):(o.debug("using HTTPS over HTTP proxy",JSON.stringify(n)),t.httpsAgent=h.httpsOverHttp({proxy:n})),t.proxy=!1}},get:async(e,s)=>{const{log:o}=i.merge({log:{debug:()=>{}}},s);let r,n=s.url,a=e;s.skip_url_mangle?r=a:(a=`/${a}`,n=n.replace(/\/+$/,""),a=a.replace(/\/+/g,"/"),r=n+a);const c={url:r,gzip:!0};let h;v.headers(c,s);try{o.debug(`GET ${c.url}`),h=await l.get(c.url,c)}catch(u){return Promise.reject(u)}return t(h,s)},post:async(e,s,o)=>{const{log:r}=i.merge({log:{debug:()=>{}}},o);let n=o.url;n=n.replace(/\/+$/,"");const a={url:`${n}/${e.replace(/\/+/g,"/")}`,gzip:!0,data:s,headers:{}};if(o.legacy_form){const e=[],t=i.merge({json:JSON.stringify(s)},s);Object.keys(t).sort().forEach(s=>{e.push(`${s}=${escape(t[s])}`)}),a.data=e.join("&"),a.headers["Content-Type"]="application/x-www-form-urlencoded"}v.headers(a,o);const{data:c}=a;let h;delete a.data;try{r.debug(`POST ${a.url}`),h=await l.post(a.url,c,a)}catch(u){return Promise.reject(u)}return o.handler?o.handler(h):t(h,o)},put:async(e,s,o,r)=>{const{log:n}=i.merge({log:{debug:()=>{}}},r);let a=r.url;a=a.replace(/\/+$/,"");const c={url:`${a}/${e.replace(/\/+/g,"/")}/${s}`,gzip:!0,data:o,headers:{}};if(r.legacy_form){const e=[],t=i.merge({json:JSON.stringify(o)},o);Object.keys(t).sort().forEach(s=>{e.push(`${s}=${escape(t[s])}`)}),c.data=e.join("&"),c.headers["Content-Type"]="application/x-www-form-urlencoded"}v.headers(c,r);const{data:h}=c;let u;delete c.data;try{n.debug(`PUT ${c.url}`),u=await l.put(c.url,h,c)}catch(d){return Promise.reject(d)}return t(u,r)}}}();v.pipe=async(e,t,i,o)=>{let r=i.url,n=`/${e}`;r=r.replace(/\/+$/,""),n=n.replace(/\/+/g,"/");const a={url:r+n,gzip:!0,headers:{"Accept-Encoding":"gzip",Accept:"application/gzip"}};return v.headers(a,i),i.proxy&&(a.proxy=i.proxy),o&&(a.onUploadProgress=o),a.responseType="stream",new Promise((e,i)=>{l.get(a.url,a).then(o=>{const r=s.createWriteStream(t);o.data.pipe(r),r.on("finish",()=>{e(t)}),r.on("error",e=>{i(new Error(`writer failed ${String(e)}`))})}).catch(e=>{i(e)})})};let I=0;v.getFileID=()=>(I+=1,`FILE_${I}`),v.lsRecursive=async(e,t,o)=>{let r=e;const a=s.statSync(t);if(o){if(await o(t,a))return null}return a.isDirectory()?s.readdir(t).then(e=>e.map(e=>n.join(t,e))).then(e=>Promise.all(e.map(e=>v.lsRecursive(r,e,o)))).then(e=>i.flatten(e)):(a.isFile()&&r===t&&(r=n.dirname(t)),[{name:n.parse(t).base,path:t,relative:t.replace(r,""),size:a.size,id:v.getFileID()}])},v.loadInputFiles=async({inputFolders:e,outputFolder:t,filetype:s},i,o)=>{let r=s;r instanceof Array||(r=[r]),r=r.map(e=>e&&0!==e.indexOf(".")?`.${e}`:e);const a=async(e,s)=>{const i=n.basename(e),a=[new Promise((t,s)=>"downloads"===i||"skip"===i||"fail"===i||"fastq_fail"===i||"tmp"===i?s(new Error(`${e} failed basic filename`)):t("basic ok")),new Promise((o,a)=>{const l=r.length?new RegExp(`(?:${r.join("|")})$`):null;return e.split(n.sep).filter(e=>e.match(/^[.]/)).length||t&&i===n.basename(t)||l&&!e.match(l)&&s.isFile()?a(new Error(`${e} failed extended filename`)):o("extended ok")}),o?new Promise((t,s)=>{o(e).then(i=>i?s(new Error(`${e} failed extraFilter`)):t("extra ok"))}):Promise.resolve("extra skip")];return Promise.all(a).then(()=>null).catch(()=>"exclude")};return(await Promise.all(e.map(e=>v.lsRecursive(e,e,a)))).reduce((e,t)=>[...e,...t.filter(e=>!!e)],[])},v.stripFile=e=>[n.dirname(e),n.basename(e)];class E{constructor(e,t,o){const r=i.merge({},t);this.options=r,this.log=o;const{idWorkflowInstance:l,inputFolders:c}=r;o.debug(`setting up ${e}/db.sqlite for ${l}`),this.db=s.mkdirp(e).then(()=>(this.log.debug(`opening ${e}/db.sqlite`),a.open(n.join(e,"db.sqlite"),{Promise:Promise}).then(async t=>{this.log.debug(`opened ${e}/db.sqlite`),await t.migrate({migrationsPath:n.join(__dirname,"..","migrations")});const s=c.map(()=>"(?)").join(",");try{return await Promise.all([t.run("INSERT INTO meta (version, idWorkflowInstance) VALUES(?, ?)",_,l),t.run(`INSERT INTO folders (folder_path) VALUES ${s}`,c)]),Promise.resolve(t)}catch(i){return this.log.error(i),Promise.reject(i)}}))).catch(e=>{throw this.log.error(e),e})}async uploadFile(e){const t=await this.db,[s,i]=v.stripFile(e);return await t.run("INSERT OR IGNORE INTO folders (folder_path) VALUES (?)",s),t.run("INSERT INTO uploads(filename, path_id) VALUES(?, (SELECT folder_id FROM folders WHERE folder_path = ?))",i,s)}async skipFile(e){const t=await this.db,[s,i]=v.stripFile(e);return await t.run("INSERT OR IGNORE INTO folders (folder_path) VALUES (?)",s),t.run("INSERT INTO skips(filename, path_id) VALUES(?, (SELECT folder_id FROM folders WHERE folder_path = ?))",i,s)}async splitFile(e,t){const s=await this.db,[i,o]=v.stripFile(e),r=v.stripFile(t)[1];return await s.run("INSERT OR IGNORE INTO folders (folder_path) VALUES (?)",i),s.run("INSERT INTO splits VALUES(?, ?, (SELECT folder_id FROM folders WHERE folder_path = ?), CURRENT_TIMESTAMP, NULL)",o,r,i)}async splitDone(e){const t=await this.db,[s,i]=v.stripFile(e);return t.run("UPDATE splits SET end=CURRENT_TIMESTAMP WHERE filename=? AND child_path_id=(SELECT folder_id FROM folders WHERE folder_path=?)",i,s)}async splitClean(){return(await this.db).all("SELECT splits.filename, folders.folder_path FROM splits INNER JOIN folders ON folders.folder_id = splits.child_path_id WHERE end IS NULL").then(e=>{if(!e)return this.log.info("no split files to clean"),Promise.resolve();this.log.info(`cleaning ${e.length} split files`),this.log.debug(`going to clean: ${e.map(e=>e.filename).join(" ")}`);const t=e.map(e=>s.unlink(n.join(e.folder_path,e.filename)).catch(()=>{}));return Promise.all(t)})}async seenUpload(e){const t=await this.db,[s,o]=v.stripFile(e);return Promise.all([t.get("SELECT * FROM uploads u INNER JOIN folders ON folders.folder_id = u.path_id WHERE u.filename=? AND folders.folder_path=? LIMIT 1",o,s),t.get("SELECT * FROM skips s INNER JOIN folders ON folders.folder_id = s.path_id WHERE s.filename=? AND folders.folder_path=? LIMIT 1",o,s)]).then(e=>i.remove(e,void 0).length)}}var P="https://epi2me.nanoporetech.com",T={local:!1,url:P,user_agent:"EPI2ME API",region:"eu-west-1",sessionGrace:5,uploadTimeout:1200,downloadTimeout:1200,fileCheckInterval:5,downloadCheckInterval:3,stateCheckInterval:60,inFlightDelay:600,waitTimeSeconds:20,waitTokenError:30,transferPoolSize:3,downloadMode:"data+telemetry",filetype:[".fastq",".fq",".fastq.gz",".fq.gz"],signing:!0};function x(e,t,s){return t in e?Object.defineProperty(e,t,{value:s,enumerable:!0,configurable:!0,writable:!0}):e[t]=s,e}const j="\npage\npages\nhasNext\nhasPrevious\ntotalCount\n",O="\nidWorkflowInstance\nstartDate\nworkflowImage{\n  workflow\n  {\n    rev\n    name\n  }\n}\n",N=function(){const e=(e,t)=>{e.headers||(e.headers={});let s=t;if(s||(s={}),!s.apikey||!s.apisecret)return;e.headers["X-EPI2ME-APIKEY"]=s.apikey,e.headers["X-EPI2ME-SIGNATUREDATE"]=(new Date).toISOString();const i=[Object.keys(e.headers).sort().filter(e=>e.match(/^x-epi2me/i)).map(t=>`${t}:${e.headers[t]}`).join("\n"),e.body].join("\n"),o=c.createHmac("sha1",s.apisecret).update(i).digest("hex");e.headers["X-EPI2ME-SIGNATUREV0"]=o};return{version:"3.0.1630",setHeaders:(t,s)=>{const{log:o}=i.merge({log:{debug:()=>{}}},s);let r=s;if(r||(r={}),t.headers=i.merge({Accept:"application/json","Content-Type":"application/json","X-EPI2ME-CLIENT":r.user_agent||"api","X-EPI2ME-VERSION":r.agent_version||N.version},t.headers,r.headers),"signing"in r&&!r.signing||e(t,r),r.proxy){const e=r.proxy.match(/https?:\/\/((\S+):(\S+)@)?(\S+):(\d+)/),s=e[2],i=e[3],n={host:e[4],port:e[5]};s&&i&&(n.proxyAuth=`${s}:${i}`),r.proxy.match(/^https/)?(o.debug("using HTTPS over HTTPS proxy",JSON.stringify(n)),t.httpsAgent=h.httpsOverHttps({proxy:n})):(o.debug("using HTTPS over HTTP proxy",JSON.stringify(n)),t.httpsAgent=h.httpsOverHttp({proxy:n})),t.proxy=!1}}}}(),R=w.buildAxiosFetch(l),M=(e,t)=>{const{apikey:s,apisecret:i}=t.headers.keys;return delete t.headers.keys,N.setHeaders(t,{apikey:s,apisecret:i,signing:!0}),R(e,t)},F=new g({link:new f.ApolloLink(e=>{const{apikey:t,apisecret:s,url:i}=e.getContext(),o=m.createHttpLink({uri:`${i}/graphql`,fetch:M,headers:{keys:{apikey:t,apisecret:s}}});return f.execute(o,e)}),cache:new p.InMemoryCache});class q{constructor(e){x(this,"createContext",e=>{const{apikey:t,apisecret:s,url:o}=this.options;return i.merge({apikey:t,apisecret:s,url:o},e)}),x(this,"query",e=>({context:t={},variables:s={},options:i={}}={})=>{const o=this.createContext(t);let r;return r="string"===typeof e?d`
        ${e}
      `:"function"===typeof e?d`
        ${e(j)}
      `:e,this.client.query({query:r,variables:s,...i,context:o})}),x(this,"mutate",e=>({context:t={},variables:s={},options:i={}}={})=>{const o=this.createContext(t);let r;return r="string"===typeof e?d`
        ${e}
      `:e,this.client.mutate({mutation:r,variables:s,...i,context:o})}),x(this,"resetCache",()=>{this.client.resetStore()}),x(this,"workflows",this.query(d`
    query allWorkflows($page: Int, $pageSize: Int, $isActive: Int, $orderBy: String) {
      allWorkflows(page: $page, pageSize: $pageSize, isActive: $isActive, orderBy: $orderBy) {
        ${j}
        results {
          ${"\nidWorkflow\nname\ndescription\nsummary\nrev\n"}
        }
      }
    }
  `)),x(this,"workflowPages",async e=>{let t=e,s=await this.workflows({variables:{page:t}});const i=async e=>(t=e,s=await this.workflows({variables:{page:t}}),s);return{data:s,next:()=>i(t+1),previous:()=>i(t-1),first:()=>i(1),last:()=>i(0)}}),x(this,"workflow",this.query(d`
    query workflow($idWorkflow: ID!) {
      workflow(idWorkflow: $idWorkflow) {
        ${"\nidWorkflow\nname\ndescription\nsummary\nrev\n"}
      }
    }
   `)),x(this,"workflowInstances",this.query(d`
  query allWorkflowInstances($page: Int, $pageSize: Int, $shared: Boolean, $idUser: ID, $orderBy: String) {
    allWorkflowInstances(page: $page, pageSize: $pageSize, shared: $shared, idUser: $idUser, orderBy: $orderBy) {
      ${j}
      results {
        ${O}
      }
    }
  }
   `)),x(this,"workflowInstance",this.query(d`
      query workflowInstance($idWorkflowInstance: ID!) {
        workflowInstance(idWorkflowInstance: $idWorkflowInstance) {
          ${O}
        }
      }
   `)),x(this,"startWorkflow",this.mutate(d`
    mutation startWorkflow(
      $idWorkflow: ID!
      $computeAccountId: Int!
      $storageAccountId: Int
      $isConsentedHuman: Int = 0
    ) {
      startWorkflowInstance(
        idWorkflow: $idWorkflow
        computeAccountId: $computeAccountId
        storageAccountId: $storageAccountId
        isConsentedHuman: $isConsentedHuman
      ) {
        bucket
        idUser
        idWorkflowInstance
        inputqueue
        outputqueue
        region
        keyId
        chain
      }
    }
  `)),x(this,"user",this.query(d`
    query user {
      me {
        username
        realname
        useraccountSet {
          idUserAccount
        }
      }
    }
  `)),x(this,"register",this.mutate(d`
    mutation registerToken($code: String!, $description: String) {
      registerToken(code: $code, description: $description) {
        apikey
        apisecret
        description
      }
    }
  `)),this.options=i.assign({agent_version:v.version,local:!1,url:P,user_agent:"EPI2ME API",signing:!0},e),this.options.url=this.options.url.replace(/:\/\//,"://graphql."),this.log=this.options.log,this.client=F}}const A=(e,t)=>{const s=["","K","M","G","T","P","E","Z"];let i=t||0,o=e||0;return o>=1e3?(o/=1e3,i+=1,i>=s.length?"???":A(o,i)):0===i?`${o}${s[i]}`:`${o.toFixed(1)}${s[i]}`};class C{constructor(e,t){this.allProfileData={},this.defaultEndpoint=process.env.METRICHOR||T.endpoint||T.url,this.raiseExceptions=t,e&&(this.allProfileData=i.merge(e,{profiles:{}})),this.allProfileData.endpoint&&(this.defaultEndpoint=this.allProfileData.endpoint)}profile(e){return e?i.merge({endpoint:this.defaultEndpoint},this.allProfileData.profiles[e]):{}}profiles(){return Object.keys(this.allProfileData.profiles||{})}}class W{constructor(e){this.options=i.assign({agent_version:v.version,local:!1,url:P,user_agent:"EPI2ME API",signing:!0},e),this.log=this.options.log}async list(e){const t=e.match(/^[a-z_]+/i)[0];return v.get(e,this.options).then(e=>e[`${t}s`])}async read(e,t){return v.get(`${e}/${t}`,this.options)}async user(){return this.options.local?{accounts:[{id_user_account:"none",number:"NONE",name:"None"}]}:v.get("user",this.options)}async status(){return v.get("status",this.options)}async jwt(){return v.post("authenticate",{},i.merge({handler:e=>e.headers["x-epi2me-jwt"]?Promise.resolve(e.headers["x-epi2me-jwt"]):Promise.reject(new Error("failed to fetch JWT"))},this.options))}async instanceToken(e,t){return v.post("token",i.merge(t,{id_workflow_instance:e}),i.assign({},this.options,{legacy_form:!0}))}async installToken(e){return v.post("token/install",{id_workflow:e},i.assign({},this.options,{legacy_form:!0}))}async attributes(){return this.list("attribute")}async workflows(){return this.list("workflow")}async amiImages(){if(this.options.local)throw new Error("amiImages unsupported in local mode");return this.list("ami_image")}async amiImage(e,t){let s,i,o;if(e&&t instanceof Object?(s=e,i=t,o="update"):e instanceof Object&&!t?(i=e,o="create"):(o="read",s=e),this.options.local)throw new Error("ami_image unsupported in local mode");if("update"===o)return v.put("ami_image",s,i,this.options);if("create"===o)return v.post("ami_image",i,this.options);if(!s)throw new Error("no id_ami_image specified");return this.read("ami_image",s)}async workflow(e,t,s){let o,r,n,a;if(e&&t&&s instanceof Function?(o=e,r=t,n=s,a="update"):e&&t instanceof Object&&!(t instanceof Function)?(o=e,r=t,a="update"):e instanceof Object&&t instanceof Function?(r=e,n=t,a="create"):e instanceof Object&&!t?(r=e,a="create"):(a="read",o=e,n=t instanceof Function?t:null),"update"===a)try{const e=await v.put("workflow",o,r,this.options);return n?n(null,e):Promise.resolve(e)}catch(u){return n?n(u):Promise.reject(u)}if("create"===a)try{const e=await v.post("workflow",r,this.options);return n?n(null,e):Promise.resolve(e)}catch(u){return n?n(u):Promise.reject(u)}if(!o){const e=new Error("no workflow id specified");return n?n(e):Promise.reject(e)}const l={};try{const e=await this.read("workflow",o);if(e.error)throw new Error(e.error);i.merge(l,e)}catch(u){return this.log.error(`${o}: error fetching workflow ${String(u)}`),n?n(u):Promise.reject(u)}i.merge(l,{params:{}});try{const e=await v.get(`workflow/config/${o}`,this.options);if(e.error)throw new Error(e.error);i.merge(l,e)}catch(u){return this.log.error(`${o}: error fetching workflow config ${String(u)}`),n?n(u):Promise.reject(u)}const c=i.filter(l.params,{widget:"ajax_dropdown"}),h=[...c.map((e,t)=>{const s=c[t];return new Promise((e,t)=>{const i=s.values.source.replace("{{EPI2ME_HOST}}","").replace(/&?apikey=\{\{EPI2ME_API_KEY\}\}/,"");v.get(i,this.options).then(t=>{const i=t[s.values.data_root];return i&&(s.values=i.map(e=>({label:e[s.values.items.label_key],value:e[s.values.items.value_key]}))),e()}).catch(e=>(this.log.error(`failed to fetch ${i}`),t(e)))})})];try{return await Promise.all(h),n?n(null,l):Promise.resolve(l)}catch(u){return this.log.error(`${o}: error fetching config and parameters ${String(u)}`),n?n(u):Promise.reject(u)}}async startWorkflow(e){return v.post("workflow_instance",e,i.assign({},this.options,{legacy_form:!0}))}async stopWorkflow(e){return v.put("workflow_instance/stop",e,null,i.assign({},this.options,{legacy_form:!0}))}async workflowInstances(e){return e&&e.run_id?v.get(`workflow_instance/wi?show=all&columns[0][name]=run_id;columns[0][searchable]=true;columns[0][search][regex]=true;columns[0][search][value]=${e.run_id};`,this.options).then(e=>e.data.map(e=>({id_workflow_instance:e.id_ins,id_workflow:e.id_flo,run_id:e.run_id,description:e.desc,rev:e.rev}))):this.list("workflow_instance")}async workflowInstance(e){return this.read("workflow_instance",e)}async workflowConfig(e){return v.get(`workflow/config/${e}`,this.options)}async register(e,t){return v.put("reg",e,{description:t||`${r.userInfo().username}@${r.hostname()}`},i.assign({},this.options,{signing:!1}))}async datasets(e){let t=e;return t||(t={}),t.show||(t.show="mine"),this.list(`dataset?show=${t.show}`)}async dataset(e){return this.options.local?this.datasets().then(t=>t.find(t=>t.id_dataset===e)):this.read("dataset",e)}async fetchContent(e){const t=i.assign({},this.options,{skip_url_mangle:!0,headers:{"Content-Type":""}});return v.get(e,t)}}class D{constructor(e,t){this.debounces={},this.debounceWindow=i.merge({debounceWindow:2e3},t).debounceWindow,this.log=i.merge({log:{debug:()=>{}}},t).log,e.jwt().then(e=>{this.socket=y(t.url,{transportOptions:{polling:{extraHeaders:{Cookie:`x-epi2me-jwt=${e}`}}}}),this.socket.on("connect",()=>{this.log.debug("socket ready")})})}debounce(e,t){const s=i.merge(e)._uuid;if(s){if(this.debounces[s])return;this.debounces[s]=1,setTimeout(()=>{delete this.debounces[s]},this.debounceWindow)}t&&t(e)}watch(e,t){if(!this.socket)return this.log.debug(`socket not ready. requeueing watch on ${e}`),void setTimeout(()=>{this.watch(e,t)},1e3);this.socket.on(e,e=>this.debounce(e,t))}emit(e,t){if(!this.socket)return this.log.debug(`socket not ready. requeueing emit on ${e}`),void setTimeout(()=>{this.emit(e,t)},1e3);this.log.debug(`socket emit ${e} ${JSON.stringify(t)}`),this.socket.emit(e,t)}}class Q{constructor(e){let t;if(t="string"===typeof e||"object"===typeof e&&e.constructor===String?JSON.parse(e):e||{},t.endpoint&&(t.url=t.endpoint,delete t.endpoint),t.log){if(!i.every([t.log.info,t.log.warn,t.log.error,t.log.debug,t.log.json],i.isFunction))throw new Error("expected log object to have error, debug, info, warn and json methods");this.log=t.log}else this.log={info:e=>{console.info(`[${(new Date).toISOString()}] INFO: ${e}`)},debug:e=>{console.debug(`[${(new Date).toISOString()}] DEBUG: ${e}`)},warn:e=>{console.warn(`[${(new Date).toISOString()}] WARN: ${e}`)},error:e=>{console.error(`[${(new Date).toISOString()}] ERROR: ${e}`)},json:e=>{console.log(JSON.stringify(e))}};this.stopped=!0,this.uploadState$=new u.BehaviorSubject(!1),this.analyseState$=new u.BehaviorSubject(!1),this.reportState$=new u.BehaviorSubject(!1),this.instanceTelemetry$=new u.BehaviorSubject(null),this.runningStates$=u.combineLatest(this.uploadState$,this.analyseState$,this.reportState$),this.states={upload:{filesCount:0,success:{files:0,bytes:0,reads:0},types:{},niceTypes:"",progress:{bytes:0,total:0}},download:{progress:{},success:{files:0,reads:0,bytes:0},fail:0,types:{},niceTypes:""},warnings:[]},this.liveStates$=new u.BehaviorSubject(this.states),this.config={options:i.defaults(t,T),instance:{id_workflow_instance:t.id_workflow_instance,inputQueueName:null,outputQueueName:null,outputQueueURL:null,discoverQueueCache:{},bucket:null,bucketFolder:null,remote_addr:null,chain:null,key_id:null}},this.config.instance.awssettings={region:this.config.options.region},this.REST=new W(i.merge({log:this.log},this.config.options)),this.graphQL=new q(i.merge({log:this.log},this.config.options)),this.timers={downloadCheckInterval:null,stateCheckInterval:null,fileCheckInterval:null,transferTimeouts:{},visibilityIntervals:{},summaryTelemetryInterval:null}}async socket(){return this.mySocket?this.mySocket:(this.mySocket=new D(this.REST,i.merge({log:this.log},this.config.options)),this.mySocket)}async realtimeFeedback(e,t){(await this.socket()).emit(e,t)}stopTimer(e){this.timers[e]&&(this.log.debug(`clearing ${e} interval`),clearInterval(this.timers[e]),this.timers[e]=null)}async stopAnalysis(){this.stopUpload();const{id_workflow_instance:e}=this.config.instance;if(e){try{await this.REST.stopWorkflow(e),this.analyseState$.next(!1)}catch(t){return this.log.error(`Error stopping instance: ${String(t)}`),Promise.reject(t)}this.log.info(`workflow instance ${e} stopped`)}return Promise.resolve()}async stopUpload(){return this.stopped=!0,this.log.debug("stopping watchers"),["downloadCheckInterval","stateCheckInterval","fileCheckInterval"].forEach(e=>this.stopTimer(e)),this.uploadState$.next(!1),Object.keys(this.timers.transferTimeouts).forEach(e=>{this.log.debug(`clearing transferTimeout for ${e}`),clearTimeout(this.timers.transferTimeouts[e]),delete this.timers.transferTimeouts[e]}),Object.keys(this.timers.visibilityIntervals).forEach(e=>{this.log.debug(`clearing visibilityInterval for ${e}`),clearInterval(this.timers.visibilityIntervals[e]),delete this.timers.visibilityIntervals[e]}),this.downloadWorkerPool&&(this.log.debug("clearing downloadWorkerPool"),await Promise.all(Object.values(this.downloadWorkerPool)),this.downloadWorkerPool=null),Promise.resolve()}async stopEverything(){this.stopAnalysis(),this.stopTimer("summaryTelemetryInterval")}reportProgress(){const{upload:e,download:t}=this.states;this.log.json({progress:{download:t,upload:e}})}storeState(e,t,s,i){const o=i||{};this.states[e]||(this.states[e]={}),this.states[e][t]||(this.states[e][t]={}),"incr"===s?Object.keys(o).forEach(s=>{this.states[e][t][s]=this.states[e][t][s]?this.states[e][t][s]+parseInt(o[s],10):parseInt(o[s],10)}):Object.keys(o).forEach(s=>{this.states[e][t][s]=this.states[e][t][s]?this.states[e][t][s]-parseInt(o[s],10):-parseInt(o[s],10)});try{this.states[e].success.niceReads=A(this.states[e].success.reads)}catch(n){this.states[e].success.niceReads=0}try{this.states[e].progress.niceSize=A(this.states[e].success.bytes+this.states[e].progress.bytes||0)}catch(n){this.states[e].progress.niceSize=0}try{this.states[e].success.niceSize=A(this.states[e].success.bytes)}catch(n){this.states[e].success.niceSize=0}this.states[e].niceTypes=Object.keys(this.states[e].types||{}).sort().map(t=>`${this.states[e].types[t]} ${t}`).join(", ");const r=Date.now();(!this.stateReportTime||r-this.stateReportTime>2e3)&&(this.stateReportTime=r,this.reportProgress()),this.liveStates$.next({...this.states})}uploadState(e,t,s){return this.storeState("upload",e,t,s)}downloadState(e,t,s){return this.storeState("download",e,t,s)}url(){return this.config.options.url}apikey(){return this.config.options.apikey}attr(e,t){if(!(e in this.config.options))throw new Error(`config object does not contain property ${e}`);return t?(this.config.options[e]=t,this):this.config.options[e]}stats(e){return this.states[e]}}Q.version=v.version,Q.Profile=C,Q.REST=W,Q.utils=v;const z={fastq:function(e){return new Promise((t,i)=>{let o,r=1,n={size:0};try{n=s.statSync(e)}catch(a){return void i(a)}s.createReadStream(e).on("data",e=>{o=-1,r-=1;do{o=e.indexOf(10,o+1),r+=1}while(-1!==o)}).on("end",()=>t({type:"fastq",bytes:n.size,reads:Math.floor(r/4)})).on("error",i)})},fasta:function(e){return new Promise((t,i)=>{let o,r=1,n={size:0};try{n=s.statSync(e)}catch(a){i(a)}s.createReadStream(e).on("data",e=>{o=-1,r-=1;do{o=e.indexOf(62,o+1),r+=1}while(-1!==o)}).on("end",()=>t({type:"fasta",bytes:n.size,sequences:Math.floor((1+r)/2)})).on("error",i)})},default:async function(e){return s.stat(e).then(e=>({type:"bytes",bytes:e.size}))}};function U(e){if("string"!==typeof e&&!(e instanceof String))return Promise.resolve({});let t=n.extname(e).toLowerCase().replace(/^[.]/,"");return"fq"===t?t="fastq":"fa"===t&&(t="fasta"),z[t]||(t="default"),z[t](e)}class L extends C{constructor(e,t){super({},t),this.prefsFile=e||L.profilePath(),this.allProfileData={};try{this.allProfileData=i.merge(s.readJSONSync(this.prefsFile),{profiles:{}}),this.allProfileData.endpoint&&(this.defaultEndpoint=this.allProfileData.endpoint)}catch(o){if(this.raiseExceptions)throw o}}static profilePath(){return n.join(o.homedir(),".epi2me.json")}profile(e,t){if(e&&t){i.merge(this.allProfileData,{profiles:{[e]:t}});try{s.writeJSONSync(this.prefsFile,this.allProfileData)}catch(o){if(this.raiseExceptions)throw o}}return e?i.merge({endpoint:this.defaultEndpoint},this.allProfileData.profiles[e]):{}}}class J{static MakeQueryablePromise(e){if(e.isResolved)return e;let t=!0,s=!1,i=!1;const o=e.then(e=>(i=!0,t=!1,e)).catch(e=>{throw s=!0,t=!1,e});return o.dependsOn=e,o.isResolved=()=>i,o.isPending=()=>t,o.isRejected=()=>s,o}constructor(e){const t=i.merge({bandwidth:1,interval:500},e);this.bandwidth=t.bandwidth,this.interval=t.interval,this.pipeline=[],this.running=[],this.completed=0,this.intervalId=null,"start"in t&&!t.start||this.start()}enqueue(e){this.pipeline.push(e)}start(){this.intervalId||(this.intervalId=setInterval(()=>{this.monitorInterval()},this.interval))}stop(){clearInterval(this.intervalId),delete this.intervalId}state(){return{queued:this.pipeline.length,running:this.running.length,completed:this.completed,state:this.intervalId?"running":"stopped"}}monitorInterval(){this.running.map((e,t)=>e.isPending()?null:t).filter(e=>e).reverse().forEach(e=>{this.running.splice(e,1),this.completed+=1});const e=this.bandwidth-this.running.length;for(let t=0;t<e;t+=1){const e=this.pipeline.shift();if(!e)return;this.running.push(J.MakeQueryablePromise(e()))}}}class H extends W{async workflows(e){if(!this.options.local)return super.workflows(e);const t=n.join(this.options.url,"workflows");let i;try{return i=(await s.readdir(t)).filter(e=>s.statSync(n.join(t,e)).isDirectory()).map(e=>n.join(t,e,"workflow.json")).map(e=>s.readJsonSync(e)),e?e(null,i):Promise.resolve(i)}catch(o){return this.log.warn(o),e?e(void 0):Promise.reject(void 0)}}async workflow(e,t,i){if(!this.options.local||!e||"object"===typeof e||i)return super.workflow(e,t,i);const o=n.join(this.options.url,"workflows"),r=n.join(o,e,"workflow.json");try{const e=await s.readJson(r);return i?i(null,e):Promise.resolve(e)}catch(a){return i?i(a):Promise.reject(a)}}async workflowInstances(e,t){if(!this.options.local)return super.workflowInstances(e,t);let i,o;if(!e||e instanceof Function||void 0!==t?(i=e,o=t):o=e,o){const e=new Error("querying of local instances unsupported in local mode");return i?i(e):Promise.reject(e)}const r=n.join(this.options.url,"instances");try{let e=await s.readdir(r);return e=e.filter(e=>s.statSync(n.join(r,e)).isDirectory()),e=e.map(e=>{const t=n.join(r,e,"workflow.json");let i;try{i=s.readJsonSync(t)}catch(o){i={id_workflow:"-",description:"-",rev:"0.0"}}return i.id_workflow_instance=e,i.filename=t,i}),i?i(null,e):Promise.resolve(e)}catch(a){return i?i(a):Promise.reject(a)}}async datasets(e,t){if(!this.options.local)return super.datasets(e,t);let i,o;if(!e||e instanceof Function||void 0!==t?(i=e,o=t):o=e,o||(o={}),o.show||(o.show="mine"),"mine"!==o.show)return i(new Error("querying of local datasets unsupported in local mode"));const r=n.join(this.options.url,"datasets");try{let e=await s.readdir(r);e=e.filter(e=>s.statSync(n.join(r,e)).isDirectory());let t=0;return e=e.sort().map(e=>(t+=1,{is_reference_dataset:!0,summary:null,dataset_status:{status_label:"Active",status_value:"active"},size:0,prefix:e,id_workflow_instance:null,id_account:null,is_consented_human:null,data_fields:null,component_id:null,uuid:e,is_shared:!1,id_dataset:t,id_user:null,last_modified:null,created:null,name:e,source:e,attributes:null})),i?i(null,e):Promise.resolve(e)}catch(a){return this.log.warn(a),i?i(null,[]):Promise.resolve([])}}async bundleWorkflow(e,t,s){return v.pipe(`workflow/bundle/${e}.tar.gz`,t,this.options,s)}}class B{constructor(e){this.experiments={},this.options=i.assign({path:"/data"},e)}async getExperiments(e=!1){return Object.keys(this.experiments).length&&!e||await this.updateExperiments(),this.experiments}async updateExperiments(){const e=await k.list(this.options.path,{include:["sequencing_summary"]});this.experiments={},e.forEach(e=>{const[t,s]=i.takeRight(e.path.split(n.sep),2),o=s.split("_"),r=i.takeRight(o,2)[0],[a,l]=o.slice(0,2),c=`${a.slice(0,4)}-${a.slice(4,6)}-${a.slice(6,8)}`,h=`T${l.slice(0,2)}:${l.slice(2,4)}:00`,u=new Date(c+h);this.experiments[t]={startDate:`${u.toDateString()} ${u.toLocaleTimeString()}`,samples:[...this.experiments[t]?this.experiments[t].samples:[],{sample:s,flowcell:r,path:`${e.path}/fastq_pass`}]}})}}class G{constructor(e,t,s,o){if(this.id_workflow_instance=e,this.children=s,this.options=i.merge(o),this.log=this.options.log,this.REST=t,!e)throw new Error("must specify id_workflow_instance");if(!s||!s.length)throw new Error("must specify children to session")}async session(){if(this.sts_expiration&&this.sts_expiration>Date.now())return Promise.resolve();this.log.debug("new instance token needed");try{const e=await this.REST.instanceToken(this.id_workflow_instance,this.options);this.log.debug(`allocated new instance token expiring at ${e.expiration}`),this.sts_expiration=new Date(e.expiration).getTime()-60*parseInt(this.options.sessionGrace||"0",10);const t={};this.options.proxy&&i.merge(t,{httpOptions:{agent:S(this.options.proxy,!0)}}),i.merge(t,{region:this.options.region},e),this.children.forEach(e=>{try{e.config.update(t)}catch(s){this.log.warn(`failed to update config on ${String(e)}: ${String(s)}`)}})}catch(e){this.log.warn(`failed to fetch instance token: ${String(e)}`)}return Promise.resolve()}}async function V(e,t,o,r,a,l){const{maxChunkBytes:c,maxChunkReads:h}=i.merge({},t),u=n.dirname(e),d=n.basename(e),p=d.match(/^[^.]+/)[0],g=d.replace(p,""),f=n.join(u,p);if(!c&&!h)return o(e).then(()=>({source:e,split:!1,chunks:[e]}));const m=await s.stat(e);return c&&m.size<c?o(e).then(()=>({source:e,split:!1,chunks:[e]})):new Promise(t=>{let n,u,d=0,p=0,m="",w=0,y=0;const k={source:e,split:!0,chunks:[]};let S;const b=[new Promise(e=>{S=e})];$.createInterface({input:a(e)}).on("line",async e=>{p+=1,m+=e,m+="\n",p>=4&&(p=0,(async e=>{if(!w){d+=1,n=`${f}_${d}${g}`;const e=new Promise((e,t)=>{const i=n,a=()=>{o(i).then(()=>{e(i)}).catch(e=>{t(e)}).finally(()=>{s.unlink(i).catch(e=>{r.warn(`Error unlinking chunk ${i}: ${String(e)}`)})})};l?u=l(i,a):(u=s.createWriteStream(i),u.on("close",a))});b.push(e)}w+=1,y+=e.length,u.write(e,()=>{}),(c&&y>=c||h&&w>=h)&&(w=0,y=0,u.end())})(m),m="")}).on("close",()=>{u.end(),S(),Promise.all(b).then(e=>{e.shift(),t(i.merge({chunks:e},k))})}).on("error",t=>{r.error(`Error chunking ${e}: ${String(t)}`)})})}async function K(e,t,i,o){return V(e,t,i,o,e=>s.createReadStream(e))}async function X(e,t,i,o){return V(e,t,i,o,e=>s.createReadStream(e).pipe(b.createGunzip()),(e,t)=>{const i=s.createWriteStream(e);i.on("close",t);const o=b.createGzip();return o.pipe(i),o})}const Y=()=>{const e=process.env.APPDATA||("darwin"===process.platform?n.join(o.homedir(),"Library/Application Support"):o.homedir());return process.env.EPI2ME_HOME||n.join(e,"linux"===process.platform?".epi2me":"EPI2ME")};class Z extends Q{constructor(e){super(e),this.config.options.inputFolders=this.config.options.inputFolders||[],this.config.options.inputFolder&&this.config.options.inputFolders.push(this.config.options.inputFolder),this.REST=new H(i.merge({},{log:this.log},this.config.options)),this.SampleReader=new B}async sessionedS3(){return await this.sessionManager.session(),new t.S3({useAccelerateEndpoint:"on"===this.config.options.awsAcceleration})}async sessionedSQS(){return await this.sessionManager.session(),new t.SQS}async deleteMessage(e){try{const t=await this.discoverQueue(this.config.instance.outputQueueName);return(await this.sessionedSQS()).deleteMessage({QueueUrl:t,ReceiptHandle:e.ReceiptHandle}).promise()}catch(t){return this.log.error(`deleteMessage exception: ${String(t)}`),this.states.download.failure||(this.states.download.failure={}),this.states.download.failure[t]=this.states.download.failure[t]?this.states.download.failure[t]+1:1,Promise.reject(t)}}async discoverQueue(e){if(this.config.instance.discoverQueueCache[e])return Promise.resolve(this.config.instance.discoverQueueCache[e]);let t;this.log.debug(`discovering queue for ${e}`);try{const s=await this.sessionedSQS();t=await s.getQueueUrl({QueueName:e}).promise()}catch(s){return this.log.error(`Error: failed to find queue for ${e}: ${String(s)}`),Promise.reject(s)}return this.log.debug(`found queue ${t.QueueUrl}`),this.config.instance.discoverQueueCache[e]=t.QueueUrl,Promise.resolve(t.QueueUrl)}async queueLength(e){if(!e)return Promise.reject(new Error("no queueURL specified"));const t=e.match(/([\w\-_]+)$/)[0];this.log.debug(`querying queue length of ${t}`);try{const t=await this.sessionedSQS(),s=await t.getQueueAttributes({QueueUrl:e,AttributeNames:["ApproximateNumberOfMessages"]}).promise();if(s&&s.Attributes&&"ApproximateNumberOfMessages"in s.Attributes){let e=s.Attributes.ApproximateNumberOfMessages;return e=parseInt(e,10)||0,Promise.resolve(e)}return Promise.reject(new Error("unexpected response"))}catch(s){return this.log.error(`error in getQueueAttributes ${String(s)}`),Promise.reject(s)}}async autoStart(e,t){let s;this.stopped=!1;try{s=await this.REST.startWorkflow(e),this.analyseState$.next(!0)}catch(i){const e=`Failed to start workflow: ${String(i)}`;return this.log.warn(e),t?t(e):Promise.reject(i)}return this.config.workflow=JSON.parse(JSON.stringify(e)),this.log.info(`instance ${JSON.stringify(s)}`),this.log.info(`workflow config ${JSON.stringify(this.config.workflow)}`),this.autoConfigure(s,t)}async autoJoin(e,t){let s;this.stopped=!1,this.config.instance.id_workflow_instance=e;try{s=await this.REST.workflowInstance(e)}catch(i){const e=`Failed to join workflow instance: ${String(i)}`;return this.log.warn(e),t?t(e):Promise.reject(i)}return"stopped"===s.state?(this.log.warn(`workflow ${e} is already stopped`),t?t("could not join workflow"):Promise.reject(new Error("could not join workflow"))):(this.config.workflow=this.config.workflow||{},this.log.debug(`instance ${JSON.stringify(s)}`),this.log.debug(`workflow config ${JSON.stringify(this.config.workflow)}`),this.autoConfigure(s,t))}initSessionManager(e,s){return new G(this.config.instance.id_workflow_instance,this.REST,[t,...s||[]],i.merge({sessionGrace:this.config.options.sessionGrace,proxy:this.config.options.proxy,region:this.config.instance.region,log:this.log},e))}async autoConfigure(e,t){if(["id_workflow_instance","id_workflow","remote_addr","key_id","bucket","user_defined","start_date","id_user"].forEach(t=>{this.config.instance[t]=e[t]}),this.config.instance.inputQueueName=e.inputqueue,this.config.instance.outputQueueName=e.outputqueue,this.config.instance.region=e.region||this.config.options.region,this.config.instance.bucketFolder=`${e.outputqueue}/${e.id_user}/${e.id_workflow_instance}`,this.config.instance.summaryTelemetry=e.telemetry,e.chain)if("object"===typeof e.chain)this.config.instance.chain=e.chain;else try{this.config.instance.chain=JSON.parse(e.chain)}catch(c){throw new Error(`exception parsing chain JSON ${String(c)}`)}if(!this.config.options.inputFolders.length)throw new Error("must set inputFolder");if(!this.config.options.outputFolder)throw new Error("must set outputFolder");if(!this.config.instance.bucketFolder)throw new Error("bucketFolder must be set");if(!this.config.instance.inputQueueName)throw new Error("inputQueueName must be set");if(!this.config.instance.outputQueueName)throw new Error("outputQueueName must be set");s.mkdirpSync(this.config.options.outputFolder);const i=n.join(Y(),"instances"),o=n.join(i,this.config.instance.id_workflow_instance);this.db=new E(o,{idWorkflowInstance:this.config.instance.id_workflow_instance,inputFolders:this.config.options.inputFolders},this.log);const r=this.config.instance.id_workflow_instance?`telemetry-${this.config.instance.id_workflow_instance}.log`:"telemetry.log",a=n.join(this.config.options.outputFolder,"epi2me-logs"),l=n.join(a,r);return s.mkdirp(a,e=>{if(e&&!String(e).match(/EEXIST/))this.log.error(`error opening telemetry log stream: mkdirpException:${String(e)}`);else try{this.telemetryLogStream=s.createWriteStream(l,{flags:"a"}),this.log.info(`logging telemetry to ${l}`)}catch(t){this.log.error(`error opening telemetry log stream: ${String(t)}`)}}),t&&t(null,this.config.instance),this.timers.summaryTelemetryInterval=setInterval(()=>{this.stopped?clearInterval(this.timers.summaryTelemetryInterval):this.fetchTelemetry()},1e4*this.config.options.downloadCheckInterval),this.timers.downloadCheckInterval=setInterval(()=>{this.stopped?clearInterval(this.timers.downloadCheckInterval):this.checkForDownloads()},1e3*this.config.options.downloadCheckInterval),this.timers.stateCheckInterval=setInterval(async()=>{if(this.stopped)clearInterval(this.timers.stateCheckInterval);else try{const t=await this.REST.workflowInstance(this.config.instance.id_workflow_instance);if("stopped"===t.state){this.log.warn(`instance was stopped remotely at ${t.stop_date}. shutting down the workflow.`);try{const e=await this.stopEverything();"function"===typeof e.config.options.remoteShutdownCb&&e.config.options.remoteShutdownCb(`instance was stopped remotely at ${t.stop_date}`)}catch(e){this.log.error(`Error whilst stopping: ${String(e)}`)}}}catch(t){this.log.warn(`failed to check instance state: ${t&&t.error?t.error:t}`)}},1e3*this.config.options.stateCheckInterval),this.sessionManager=this.initSessionManager(),await this.sessionManager.session(),this.reportProgress(),this.loadUploadFiles(),this.uploadState$.next(!0),this.timers.fileCheckInterval=setInterval(this.loadUploadFiles.bind(this),1e3*this.config.options.fileCheckInterval),Promise.resolve(e)}async stopUpload(){return await super.stopUpload(),this.log.debug("clearing split files"),this.db?this.db.splitClean():(delete this.sessionManager,Promise.resolve())}async stopEverything(){await super.stopEverything()}async checkForDownloads(){if(this.checkForDownloadsRunning)return Promise.resolve();this.checkForDownloadsRunning=!0,this.log.debug("checkForDownloads checking for downloads");try{const e=await this.discoverQueue(this.config.instance.outputQueueName),t=await this.queueLength(e);t?(this.log.debug(`downloads available: ${t}`),await this.downloadAvailable()):this.log.debug("no downloads available")}catch(e){this.log.warn(`checkForDownloads error ${String(e)}`),this.states.download.failure||(this.states.download.failure={}),this.states.download.failure[e]=this.states.download.failure[e]?this.states.download.failure[e]+1:1}return this.checkForDownloadsRunning=!1,Promise.resolve()}async downloadAvailable(){const e=Object.keys(this.downloadWorkerPool||{}).length;if(e>=this.config.options.transferPoolSize)return this.log.debug(`${e} downloads already queued`),Promise.resolve();let t;try{const s=await this.discoverQueue(this.config.instance.outputQueueName);this.log.debug("fetching messages");const i=await this.sessionedSQS();t=await i.receiveMessage({AttributeNames:["All"],QueueUrl:s,VisibilityTimeout:this.config.options.inFlightDelay,MaxNumberOfMessages:this.config.options.transferPoolSize-e,WaitTimeSeconds:this.config.options.waitTimeSeconds}).promise()}catch(s){return this.log.error(`receiveMessage exception: ${String(s)}`),this.states.download.failure[s]=this.states.download.failure[s]?this.states.download.failure[s]+1:1,Promise.reject(s)}return this.receiveMessages(t)}async loadUploadFiles(){if(this.dirScanInProgress)return Promise.resolve();this.dirScanInProgress=!0,this.log.debug("upload: started directory scan");try{const e=e=>this.db.seenUpload(e),t=await v.loadInputFiles(this.config.options,this.log,e);let s=0;const i=()=>new Promise(e=>{if(this.stopped)return t.length=0,this.log.debug("upload: skipping, stopped"),void e();if(s>this.config.options.transferPoolSize)return void setTimeout(e,1e3);const i=t.splice(0,this.config.options.transferPoolSize-s);s+=i.length,this.enqueueUploadFiles(i).then().catch(e=>{this.log.error(`upload: exception in enqueueUploadFiles: ${String(e)}`)}).finally(()=>{s-=i.length,e()})});for(;t.length;)await i()}catch(e){this.log.error(`upload: exception in loadInputFiles: ${String(e)}`)}return this.dirScanInProgress=!1,this.log.debug("upload: finished directory scan"),Promise.resolve()}async enqueueUploadFiles(e){let t=0,s=0,o=0,r=0,a={};if(!i.isArray(e)||!e.length)return Promise.resolve();if(this.log.info(`enqueueUploadFiles ${e.length} files: ${e.map(e=>e.path).join(" ")}.`),"workflow"in this.config)if("workflow_attributes"in this.config.workflow)a=this.config.workflow.workflow_attributes;else if("attributes"in this.config.workflow){let{attributes:e}=this.config.workflow;if(e||(e={}),["max_size","max_files","split_size","split_reads"].forEach(t=>{`epi2me:${t}`in e&&(a[t]=parseInt(e[`epi2me:${t}`],10))}),"epi2me:category"in e){e["epi2me:category"].includes("storage")&&(a.requires_storage=!0)}}if(this.log.info(`enqueueUploadFiles settings ${JSON.stringify(a)}`),"requires_storage"in a&&a.requires_storage&&!("storage_account"in this.config.workflow)){const e={msg:"ERROR: Workflow requires storage enabled. Please provide a valid storage account [ --storage ].",type:"WARNING_STORAGE_ENABLED"};return this.log.error(e.msg),this.states.warnings.push(e),Promise.resolve()}if("split_size"in a&&(o=parseInt(a.split_size,10),this.log.info(`enqueueUploadFiles splitting supported files at ${o} bytes`)),"split_reads"in a&&(r=parseInt(a.split_reads,10),this.log.info(`enqueueUploadFiles splitting supported files at ${r} reads`)),"max_size"in a&&(s=parseInt(a.max_size,10),this.log.info(`enqueueUploadFiles restricting file size to ${s}`)),"max_files"in a&&(t=parseInt(a.max_files,10),this.log.info(`enqueueUploadFiles restricting file count to ${t}`),e.length>t)){const s={msg:`ERROR: ${e.length} files found. Workflow can only accept ${t}. Please move the extra files away.`,type:"WARNING_FILE_TOO_MANY"};return this.log.error(s.msg),this.states.warnings.push(s),Promise.resolve()}this.states.upload.filesCount+=e.length;const l=e.map(async e=>{const i=e;if(t&&this.states.upload.filesCount>t){const e=`Maximum ${t} file(s) already uploaded. Marking ${i.relative} as skipped.`,s={msg:e,type:"WARNING_FILE_TOO_MANY"};this.log.error(e),this.states.warnings.push(s),this.states.upload.filesCount-=1,i.skip="SKIP_TOO_MANY"}else if(0===i.size){const e=`The file "${i.relative}" is empty. It will be skipped.`,t={msg:e,type:"WARNING_FILE_EMPTY"};i.skip="SKIP_EMPTY",this.states.upload.filesCount-=1,this.log.error(e),this.states.warnings.push(t)}else{if(i.path&&i.path.match(/\.(?:fastq|fq)(?:\.gz)?$/)&&(o&&i.size>o||r)){const e=`${i.relative}${i.size>o?" is too big and":""} is going to be split`;this.log.warn(e);const t={msg:e,type:"WARNING_FILE_SPLIT"};this.states.warnings.push(t);const s=o?{maxChunkBytes:o}:{maxChunkReads:r},l=i.path.match(/\.gz$/)?X:K,c=v.getFileID(),h=new J({bandwidth:this.config.options.transferPoolSize});let u=0;const d=async e=>(this.log.debug(`chunkHandler for ${e}`),await this.db.splitFile(e,i.path),this.stopped?(h.stop(),this.log.info(`stopped, so skipping ${e}`),Promise.reject(new Error("stopped"))):(u+=1,U(e).then(t=>({name:n.basename(e),path:e,relative:e.replace(this.config.options.inputFolder,""),id:`${c}_${u}`,stats:t,size:t.bytes})).then(async e=>{const t=new Promise(t=>{h.enqueue(()=>(this.log.info(`chunk upload starting ${e.id} ${e.path}`),this.stopped?(this.log.info(`chunk upload skipped (stopped) ${e.id} ${e.path}`),h.stop(),t(),Promise.resolve()):this.uploadJob(e).then(()=>this.db.splitDone(e.path)).catch(t=>{this.log.error(`chunk upload failed ${e.id} ${e.path}: ${String(t)}`)}).finally(t)))});await t})));try{await l(i.path,s,d,this.log),h.stop()}catch(a){if(h.stop(),"Error: stopped"===String(a))return Promise.resolve();throw a}return this.db.uploadFile(i.path)}if(s&&i.size>s){const e=`The file "${i.relative}" is bigger than the maximum size limit (${A(s)}B). It will be skipped.`,t={msg:e,type:"WARNING_FILE_TOO_BIG"};i.skip="SKIP_TOO_BIG",this.states.upload.filesCount-=1,this.log.error(e),this.states.warnings.push(t)}else try{i.stats=await U(i.path)}catch(l){this.log.error(`failed to stat ${i.path}: ${String(l)}`)}}return this.uploadJob(i)});try{return await Promise.all(l),this.log.info(`upload: inputBatchQueue (${l.length} jobs) complete`),this.loadUploadFiles()}catch(c){return this.log.error(`upload: enqueueUploadFiles exception ${String(c)}`),Promise.reject(c)}}async uploadJob(e){if("skip"in e)return this.db.skipFile(e.path);let t,s;try{this.log.info(`upload: ${e.id} starting`),t=await this.uploadHandler(e),this.log.info(`upload: ${t.id} uploaded and notified`)}catch(o){s=o,this.log.error(`upload: ${e.id} done, but failed: ${String(s)}`)}if(t||(t={}),s)this.log.error(`uploadJob ${s}`),this.states.upload.failure||(this.states.upload.failure={}),this.states.upload.failure[s]=this.states.upload.failure[s]?this.states.upload.failure[s]+1:1;else if(this.uploadState("success","incr",i.merge({files:1},t.stats)),t.name){const e=n.extname(t.name);this.uploadState("types","incr",{[e]:1})}return Promise.resolve()}async receiveMessages(e){return e&&e.Messages&&e.Messages.length?(this.downloadWorkerPool||(this.downloadWorkerPool={}),e.Messages.forEach(e=>{this.downloadWorkerPool[e.MessageId]=1;const t=setTimeout(()=>{throw this.log.error(`this.downloadWorkerPool timeoutHandle. Clearing queue slot for message: ${e.MessageId}`),new Error("download timed out")},1e3*(60+this.config.options.downloadTimeout));this.processMessage(e).catch(e=>{this.log.error(`processMessage ${String(e)}`)}).finally(()=>{clearTimeout(t),e&&delete this.downloadWorkerPool[e.MessageId]})}),this.log.info(`downloader queued ${e.Messages.length} messages for processing`),Promise.resolve()):(this.log.info("complete (empty)"),Promise.resolve())}async processMessage(e){let t,r;if(!e)return this.log.debug("download.processMessage: empty message"),Promise.resolve();"Attributes"in e&&"ApproximateReceiveCount"in e.Attributes&&this.log.debug(`download.processMessage: ${e.MessageId} / ${e.Attributes.ApproximateReceiveCount}`);try{t=JSON.parse(e.Body)}catch(h){this.log.error(`error parsing JSON message.Body from message: ${JSON.stringify(e)} ${String(h)}`);try{await this.deleteMessage(e)}catch(u){this.log.error(`Exception deleting message: ${String(u)}`)}return Promise.resolve()}if(t.telemetry){const{telemetry:s}=t;if(s.tm_path)try{this.log.debug(`download.processMessage: ${e.MessageId} fetching telemetry`);const i=await this.sessionedS3(),o=await i.getObject({Bucket:t.bucket,Key:s.tm_path}).promise();this.log.info(`download.processMessage: ${e.MessageId} fetched telemetry`),s.batch=o.Body.toString("utf-8").split("\n").filter(e=>e&&e.length>0).map(e=>{try{return JSON.parse(e)}catch(u){return this.log.error(`Telemetry Batch JSON Parse error: ${String(u)}`),e}})}catch(d){this.log.error(`Could not fetch telemetry JSON: ${String(d)}`)}try{this.telemetryLogStream.write(JSON.stringify(s)+o.EOL)}catch(p){this.log.error(`error writing telemetry: ${p}`)}this.config.options.telemetryCb&&this.config.options.telemetryCb(s)}if(!t.path)return this.log.warn("nothing to download"),Promise.resolve();const a=t.path.match(/[\w\W]*\/([\w\W]*?)$/),l=a?a[1]:"";if(r=this.config.options.outputFolder,t.telemetry&&t.telemetry.hints&&t.telemetry.hints.folder){this.log.debug(`using folder hint ${t.telemetry.hints.folder}`);const e=t.telemetry.hints.folder.split("/").map(e=>e.toUpperCase());r=n.join.apply(null,[r,...e])}s.mkdirpSync(r);const c=n.join(r,l);if("data+telemetry"===this.config.options.downloadMode){const s=[""];let i=this.config&&this.config.workflow&&this.config.workflow.settings&&this.config.workflow.settings.output_format?this.config.workflow.settings.output_format:[];("string"===typeof i||i instanceof String)&&(i=i.trim().split(/[\s,]+/));try{s.push(...i)}catch(u){this.log.error(`Failed to work out workflow file suffixes: ${String(u)}`)}try{const i=s.map(s=>{const i=t.path+s,o=c+s;return this.log.debug(`download.processMessage: ${e.MessageId} downloading ${i} to ${o}`),new Promise((r,n)=>{this.initiateDownloadStream({bucket:t.bucket,path:i},e,o).then(r).catch(e=>{this.log.error(`Caught exception waiting for initiateDownloadStream: ${String(e)}`),s?n(e):r()})})});await Promise.all(i)}catch(u){this.log.error(`Exception fetching file batch: ${String(u)}`)}try{const e=!(!t.telemetry||!t.telemetry.json)&&t.telemetry.json.exit_status;e&&this.config.options.dataCb&&this.config.options.dataCb(c,e)}catch(d){this.log.warn(`failed to fire data callback: ${d}`)}}else{const e=t.telemetry.batch_summary&&t.telemetry.batch_summary.reads_num?t.telemetry.batch_summary.reads_num:1;this.downloadState("success","incr",{files:1,reads:e})}try{await this.deleteMessage(e)}catch(u){this.log.error(`Exception deleting message: ${String(u)}`)}return this.realtimeFeedback("workflow_instance:state",{type:"stop",id_workflow_instance:this.config.instance.id_workflow_instance,id_workflow:this.config.instance.id_workflow,component_id:"0",message_id:i.merge(e).MessageId,id_user:this.config.instance.id_user}).catch(e=>{this.log.warn(`realtimeFeedback failed: ${String(e)}`)}),Promise.resolve()}async initiateDownloadStream(e,t,o){return new Promise(async(r,a)=>{let l,c,h;try{l=await this.sessionedS3()}catch(p){a(p)}const u=t=>{if(this.log.error(`Error during stream of bucket=${e.bucket} path=${e.path} to file=${o} ${String(t)}`),clearTimeout(this.timers.transferTimeouts[o]),delete this.timers.transferTimeouts[o],!c.networkStreamError)try{c.networkStreamError=1,c.close(),s.remove(o).then(()=>{this.log.warn(`removed failed download ${o}`)}).catch(e=>{this.log.warn(`failed to remove ${o}. unlinkException: ${String(e)}`)}),h.destroy&&(this.log.error(`destroying read stream for ${o}`),h.destroy())}catch(p){this.log.error(`error handling stream error: ${String(p)}`)}};try{const t={Bucket:e.bucket,Key:e.path};c=s.createWriteStream(o);const i=l.getObject(t);i.on("httpHeaders",(e,t)=>{this.downloadState("progress","incr",{total:parseInt(t["content-length"],10)})}),h=i.createReadStream()}catch(g){return this.log.error(`getObject/createReadStream exception: ${String(g)}`),void a(g)}h.on("error",u),c.on("finish",async()=>{if(!c.networkStreamError){this.log.debug(`downloaded ${o}`);try{const e=n.extname(o),t=await U(o);this.downloadState("success","incr",i.merge({files:1},t)),this.downloadState("types","incr",{[e]:1}),this.downloadState("progress","decr",{total:t.bytes,bytes:t.bytes})}catch(e){this.log.warn(`failed to stat ${o}: ${String(e)}`)}this.reportProgress()}}),c.on("close",s=>{this.log.debug(`closing writeStream ${o}`),s&&this.log.error(`error closing write stream ${s}`),clearInterval(this.timers.visibilityIntervals[o]),delete this.timers.visibilityIntervals[o],clearTimeout(this.timers.transferTimeouts[o]),delete this.timers.transferTimeouts[o],setTimeout(this.checkForDownloads.bind(this)),this.log.info(`download.initiateDownloadStream: ${t.MessageId} downloaded ${e.path} to ${o}`),r()}),c.on("error",u);const d=()=>{u(new Error("transfer timed out"))};this.timers.transferTimeouts[o]=setTimeout(d,1e3*this.config.options.downloadTimeout);this.timers.visibilityIntervals[o]=setInterval(async()=>{this.stopped&&(clearInterval(this.timers.visibilityIntervals[o]),delete this.timers.visibilityIntervals[o]);const e=this.config.instance.outputQueueURL,s=t.ReceiptHandle;this.log.debug({message_id:t.MessageId},"updateVisibility");try{await this.sqs.changeMessageVisibility({QueueUrl:e,ReceiptHandle:s,VisibilityTimeout:this.config.options.inFlightDelay}).promise()}catch(i){this.log.error({message_id:t.MessageId,queue:e,error:i},"Error setting visibility"),clearInterval(this.timers.visibilityIntervals[o])}},900*this.config.options.inFlightDelay),h.on("data",e=>{clearTimeout(this.timers.transferTimeouts[o]),this.timers.transferTimeouts[o]=setTimeout(d,1e3*this.config.options.downloadTimeout),this.downloadState("progress","incr",{bytes:e.length})}).pipe(c)})}async uploadHandler(e){const t=await this.sessionedS3();let i;const o=e.relative.replace(/^[\\/]+/,"").replace(/\\/g,"/").replace(/\//g,"_"),r=[this.config.instance.bucketFolder,"component-0",o,o].join("/").replace(/\/+/g,"/");let n;return new Promise((o,a)=>{const l=()=>{i&&!i.closed&&i.close(),a(new Error(`${e.name} timed out`))};n=setTimeout(l,1e3*(this.config.options.uploadTimeout+5));try{i=s.createReadStream(e.path)}catch(c){return clearTimeout(n),void a(c)}i.on("error",e=>{i.close();let t="error in upload readstream";e&&e.message&&(t+=`: ${e.message}`),clearTimeout(n),a(new Error(t))}),i.on("open",()=>{const s={Bucket:this.config.instance.bucket,Key:r,Body:i};this.config.instance.key_id&&(s.SSEKMSKeyId=this.config.instance.key_id,s.ServerSideEncryption="aws:kms"),e.size&&(s["Content-Length"]=e.size),this.uploadState("progress","incr",{total:e.size});let c=0;const h=t.upload(s,{partSize:10485760,queueSize:1}),u=this.initSessionManager(null,[h.service]);u.sts_expiration=this.sessionManager.sts_expiration,h.on("httpUploadProgress",async e=>{if(this.stopped)a(new Error("stopped"));else{this.uploadState("progress","incr",{bytes:e.loaded-c}),c=e.loaded,clearTimeout(n),n=setTimeout(l,1e3*(this.config.options.uploadTimeout+5));try{await u.session()}catch(t){this.log.warn(`Error refreshing token: ${String(t)}`)}}}),h.promise().then(()=>{this.log.info(`${e.id} S3 upload complete`),i.close(),clearTimeout(n),this.uploadComplete(r,e).then(()=>{o(e)}).catch(e=>{a(e)}).finally(()=>{this.uploadState("progress","decr",{total:e.size,bytes:e.size})})}).catch(t=>{this.log.warn(`${e.id} uploadStreamError ${t}`),a(t)})})})}async uploadComplete(e,t){this.log.info(`${t.id} uploaded to S3: ${e}`);const s={bucket:this.config.instance.bucket,outputQueue:this.config.instance.outputQueueName,remote_addr:this.config.instance.remote_addr,user_defined:this.config.instance.user_defined||null,apikey:this.config.options.apikey,id_workflow_instance:this.config.instance.id_workflow_instance,id_master:this.config.instance.id_workflow,utc:(new Date).toISOString(),path:e,prefix:e.substring(0,e.lastIndexOf("/"))};if(this.config.instance.chain)try{s.components=JSON.parse(JSON.stringify(this.config.instance.chain.components)),s.targetComponentId=this.config.instance.chain.targetComponentId}catch(r){return this.log.error(`${t.id} exception parsing components JSON ${String(r)}`),Promise.reject(r)}if(this.config.instance.key_id&&(s.key_id=this.config.instance.key_id),this.config.options.agent_address)try{s.agent_address=JSON.parse(this.config.options.agent_address)}catch(n){this.log.error(`${t.id} Could not parse agent_address ${String(n)}`)}s.components&&Object.keys(s.components).forEach(e=>{"uploadMessageQueue"===s.components[e].inputQueueName&&(s.components[e].inputQueueName=this.uploadMessageQueue),"downloadMessageQueue"===s.components[e].inputQueueName&&(s.components[e].inputQueueName=this.downloadMessageQueue)});let o={};try{const e=await this.discoverQueue(this.config.instance.inputQueueName),i=await this.sessionedSQS();this.log.info(`${t.id} sending SQS message to input queue`),o=await i.sendMessage({QueueUrl:e,MessageBody:JSON.stringify(s)}).promise()}catch(a){return this.log.error(`${t.id} exception sending SQS message: ${String(a)}`),Promise.reject(a)}return this.realtimeFeedback("workflow_instance:state",{type:"start",id_workflow_instance:this.config.instance.id_workflow_instance,id_workflow:this.config.instance.id_workflow,component_id:"0",message_id:i.merge(o).MessageId,id_user:this.config.instance.id_user}).catch(e=>{this.log.warn(`realtimeFeedback failed: ${String(e)}`)}),this.log.info(`${t.id} SQS message sent. Mark as uploaded`),this.db.uploadFile(t.path)}async fetchTelemetry(){if(!this.config||!this.config.instance||!this.config.instance.summaryTelemetry)return Promise.resolve();const e=n.join(Y(),"instances"),t=n.join(e,this.config.instance.id_workflow_instance),i=[];Object.keys(this.config.instance.summaryTelemetry).forEach(e=>{const o=this.config.instance.summaryTelemetry[e]||{},r=o[Object.keys(o)[0]];if(!r)return;const a=n.join(t,`${e}.json`);i.push(this.REST.fetchContent(r).then(e=>{s.writeJSONSync(a,e),this.reportState$.next(!0),this.instanceTelemetry$.next(e),this.log.debug(`fetched telemetry summary ${a}`)}).catch(e=>{this.log.debug(`Error fetching telemetry: ${String(e)}`)}))});let o=0;try{await Promise.all(i)}catch(r){o+=1}return o&&this.log.warn("summary telemetry incomplete"),Promise.resolve()}}Z.version=v.version,Z.REST=H,Z.utils=v,Z.SessionManager=G,Z.EPI2ME_HOME=Y(),Z.Profile=L,Z.Factory=class{constructor(e,t){this.EPI2ME=e,this.options=t,this.masterInstance=new e(this.options),this.log=this.masterInstance.log,this.REST=this.masterInstance.REST,this.graphQL=this.masterInstance.graphQL,this.SampleReader=this.masterInstance.SampleReader,this.version=this.masterInstance.version,this.utils=this.masterInstance.utils,this.runningInstances={}}async startRun(e,t){const s=new this.EPI2ME({...this.options,...e});try{const e=await s.autoStart(t);this.runningInstances[e.id_workflow_instance]=s}catch(i){this.log.error(`Experienced error starting ${String(i)}`);try{await s.stopEverything()}catch(o){this.log.error(`Also experienced error stopping ${String(o)}`)}}return s}},module.exports=Z;
