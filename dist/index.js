/**
 * Copyright Metrichor Ltd. (An Oxford Nanopore Technologies Company) 2019
 */

"use strict";function _interopDefault(e){return e&&"object"===typeof e&&"default"in e?e.default:e}var lodash=require("lodash"),fs=require("fs-extra"),fs__default=_interopDefault(fs),os=require("os"),os__default=_interopDefault(os),path=_interopDefault(require("path")),Promise$1=_interopDefault(require("core-js/features/promise")),axios=_interopDefault(require("axios")),crypto=_interopDefault(require("crypto")),tunnel=require("tunnel"),readline=_interopDefault(require("readline")),AWS=_interopDefault(require("aws-sdk")),proxy=_interopDefault(require("proxy-agent")),sqlite=_interopDefault(require("sqlite")),name="epi2me-api",version="2019.9.24-1457",license="MPL-2.0",repository="https://git.oxfordnanolabs.local/metrichor/api.git",description="API for communicating with the EPI2ME website(s)",main="dist/index.js",module$1="dist/index.es.js",dependencies={"@lifeomic/axios-fetch":"^1.4.0","@types/axios":"^0.14.0","apollo-cache-inmemory":"^1.6.2","apollo-client":"^2.6.3","apollo-link":"^1.2.12","apollo-link-context":"^1.0.18","apollo-link-http":"^1.5.15","aws-sdk":"2.488.0",axios:"^0.19.0","core-js":"^3.1.4","fs-extra":"^8.1.0",graphql:"^14.4.2","graphql-tag":"^2.10.1",lodash:"4.17.11","proxy-agent":"3.1.0",save:"^2.4.0",sqlite:"^3.0.3",tunnel:"^0.0.6",yarn:"^1.16.0"},devDependencies={"@babel/cli":"^7.5.0","@babel/core":"^7.5.0","@babel/plugin-proposal-object-rest-spread":"^7.5.0","@babel/preset-env":"^7.5.0","@babel/register":"^7.4.4","@types/bunyan":"^1.8.6","@types/rollup":"^0.54.0","@types/rollup-plugin-json":"^3.0.2","babel-eslint":"^10.0.2",bunyan:"^1.8.12",eslint:"6.0.1","eslint-config-airbnb-base":"^13.2.0","eslint-config-defaults":"9.0.0","eslint-config-prettier":"^6.0.0","eslint-plugin-babel":"^5.3.0","eslint-plugin-import":"^2.18.0","eslint-plugin-prettier":"^3.1.0",husky:"^3.0.0","lint-staged":"^9.0.2",mocha:"6.1.4",nyc:"^14.1.1",prettier:"^1.18.2","prettier-eslint":"^9.0.0",rollup:"^1.16.6","rollup-plugin-analyzer":"^3.0.1","rollup-plugin-cpy":"^2.0.0","rollup-plugin-eslint":"^7.0.0","rollup-plugin-generate-package-json":"^3.1.3","rollup-plugin-json":"^4.0.0","rollup-plugin-license":"^0.9.0","rollup-plugin-terser":"^5.1.0",sinon:"7.3.2",tmp:"0.1.0","xunit-file":"*"},browserslist=[">0.2%","not dead","not ie <= 11","not op_mini all"],scripts={"build:version":'jq ".version=\\"$(date +%Y.%-m.%-d)-${PATCH:-$(date +%-H%M)}\\"" < package.json > package.json.tmp && mv package.json.tmp package.json',"lint-js":'eslint --ignore-path .eslintignore --ignore-pattern "!**/.*" .',"fix-js":"yarn run lint-js --fix",lint:"yarn lint-js",deps:"yarn install","clean:dist":"rm -rf dist","clean:build":"rm -rf build && rm -rf dist/lib",clean:"yarn clean:build && yarn clean:dist",test:"mocha --recursive --require @babel/register test",cover:"yarn deps && yarn lint && yarn nyc --reporter=html --reporter=text mocha --recursive --require @babel/register test",build:"yarn build:dist","rollup:build":"rollup -c","rollup:watch":"rollup -cw","build:dist":"yarn build:version && yarn clean:dist && yarn rollup:build"},pkg={name:name,version:version,license:license,repository:repository,description:description,main:main,module:module$1,dependencies:dependencies,devDependencies:devDependencies,browserslist:browserslist,"lint-staged":{"*.{ts,tsx,js,jsx}":["yarn fix-ts --fix","git add --force"],"*.{json,md,graphql}":["prettier --write","git add --force"]},scripts:scripts};axios.defaults.validateStatus=e=>e<=504;const utils=function(){const e=(e,t)=>{e.headers||(e.headers={});let s=t;if(s||(s={}),!s.apikey)return;if(e.headers["X-EPI2ME-ApiKey"]=s.apikey,!s.apisecret)return;e.headers["X-EPI2ME-SignatureDate"]=(new Date).toISOString(),e.url.match(/^https:/)&&(e.url=e.url.replace(/:443/,"")),e.url.match(/^http:/)&&(e.url=e.url.replace(/:80/,""));const i=[e.url,Object.keys(e.headers).sort().filter(e=>e.match(/^x-epi2me/i)).map(t=>`${t}:${e.headers[t]}`).join("\n")].join("\n"),o=crypto.createHmac("sha1",s.apisecret).update(i).digest("hex");e.headers["X-EPI2ME-SignatureV0"]=o},t=async e=>{const t=e?e.data:null;if(!t)return Promise.reject(new Error("unexpected non-json response"));if(e&&e.status>=400){let s=`Network error ${e.status}`;return t.error&&(s=t.error),504===e.status&&(s="Please check your network connection and try again."),Promise.reject(new Error(s))}return t.error?Promise.reject(new Error(t.error)):Promise.resolve(t)};return{version:version,headers:(t,s)=>{const{log:i}=lodash.merge({log:{debug:()=>{}}},s);let o=s;if(o||(o={}),t.headers=lodash.merge({Accept:"application/json","Content-Type":"application/json","X-EPI2ME-Client":o.user_agent||"api","X-EPI2ME-Version":o.agent_version||utils.version},t.headers,o.headers),"signing"in o&&!o.signing||e(t,o),o.proxy){const e=o.proxy.match(/https?:\/\/((\S+):(\S+)@)?(\S+):(\d+)/),s=e[2],r=e[3],n={host:e[4],port:e[5]};s&&r&&(n.proxyAuth=`${s}:${r}`),o.proxy.match(/^https/)?(i.debug("using HTTPS over HTTPS proxy",JSON.stringify(n)),t.httpsAgent=tunnel.httpsOverHttps({proxy:n})):(i.debug("using HTTPS over HTTP proxy",JSON.stringify(n)),t.httpsAgent=tunnel.httpsOverHttp({proxy:n})),t.proxy=!1}},get:async(e,s)=>{const{log:i}=lodash.merge({log:{debug:()=>{}}},s);let o,r=s.url,n=e;s.skip_url_mangle?o=n:(n=`/${n}`,o=(r=r.replace(/\/+$/,""))+(n=n.replace(/\/+/g,"/")));const a={url:o,gzip:!0};let l;utils.headers(a,s);try{i.debug(`GET ${a.url}`),l=await axios.get(a.url,a)}catch(c){return Promise.reject(c)}return t(l,s)},post:async(e,s,i)=>{const{log:o}=lodash.merge({log:{debug:()=>{}}},i);let r=i.url;const n={url:`${r=r.replace(/\/+$/,"")}/${e.replace(/\/+/g,"/")}`,gzip:!0,data:s,headers:{}};if(i.legacy_form){const e=[],t=lodash.merge({json:JSON.stringify(s)},s);Object.keys(t).sort().forEach(s=>{e.push(`${s}=${escape(t[s])}`)}),n.data=e.join("&"),n.headers["Content-Type"]="application/x-www-form-urlencoded"}utils.headers(n,i);const{data:a}=n;let l;delete n.data;try{o.debug(`POST ${n.url}`),l=await axios.post(n.url,a,n)}catch(c){return Promise.reject(c)}return t(l,i)},put:async(e,s,i,o)=>{const{log:r}=lodash.merge({log:{debug:()=>{}}},o);let n=o.url;const a={url:`${n=n.replace(/\/+$/,"")}/${e.replace(/\/+/g,"/")}/${s}`,gzip:!0,data:i,headers:{}};if(o.legacy_form){const e=[],t=lodash.merge({json:JSON.stringify(i)},i);Object.keys(t).sort().forEach(s=>{e.push(`${s}=${escape(t[s])}`)}),a.data=e.join("&"),a.headers["Content-Type"]="application/x-www-form-urlencoded"}utils.headers(a,o);const{data:l}=a;let c;delete a.data;try{r.debug(`PUT ${a.url}`),c=await axios.put(a.url,l,a)}catch(u){return Promise.reject(u)}return t(c,o)}}}();utils.pipe=async(e,t,s,i)=>{let o=s.url,r=`/${e}`;const n={url:(o=o.replace(/\/+$/,""))+(r=r.replace(/\/+/g,"/")),gzip:!0,headers:{"Accept-Encoding":"gzip",Accept:"application/gzip"}};return utils.headers(n,s),s.proxy&&(n.proxy=s.proxy),i&&(n.onUploadProgress=i),n.responseType="stream",new Promise(async(e,s)=>{try{const o=fs__default.createWriteStream(t);(await axios.get(n.url,n)).data.pipe(o),o.on("finish",()=>{e(t)}),o.on("error",e=>{s(new Error(`writer failed ${String(e)}`))})}catch(i){s(i)}})};let IdCounter=0;utils.getFileID=()=>`FILE_${IdCounter+=1}`,utils.lsRecursive=async(e,t,s)=>{let i=e;const o=fs__default.statSync(t);if(s){if(await s(t,o))return null}return o.isDirectory()?fs__default.readdir(t).then(e=>e.map(e=>path.join(t,e))).then(e=>Promise.all(e.map(e=>utils.lsRecursive(i,e,s)))).then(e=>lodash.flatten(e)):(o.isFile()&&i===t&&(i=path.dirname(t)),[{name:path.parse(t).base,path:t,relative:t.replace(i,""),size:o.size,id:utils.getFileID()}])},utils.loadInputFiles=async({inputFolder:e,outputFolder:t,filetype:s},i,o)=>{let r=s;r instanceof Array||(r=[r]),r=r.map(e=>e&&0!==e.indexOf(".")?`.${e}`:e);const n=await utils.lsRecursive(e,e,async(e,s)=>{const i=path.basename(e),n=[new Promise((t,s)=>"downloads"===i||"skip"===i||"fail"===i||"fastq_fail"===i||"tmp"===i?s(new Error(`${e} failed basic filename`)):t("basic ok")),new Promise((o,n)=>{const a=r.length?new RegExp(`(?:${r.join("|")})$`):null;return e.split(path.sep).filter(e=>e.match(/^[.]/)).length||t&&i===path.basename(t)||a&&!e.match(a)&&s.isFile()?n(new Error(`${e} failed extended filename`)):o("extended ok")}),o?new Promise((t,s)=>{o(e).then(i=>i?s(new Error(`${e} failed extraFilter`)):t("extra ok"))}):Promise.resolve("extra skip")];return Promise.all(n).then(()=>null).catch(()=>"exclude")});return Promise.resolve(lodash.remove(n,null))};var local=!1,url="https://epi2me.nanoporetech.com",gqlUrl="https://graphql.epi2me-dev.nanoporetech.com",user_agent="EPI2ME API",region="eu-west-1",sessionGrace=5,uploadTimeout=1200,downloadTimeout=1200,fileCheckInterval=5,downloadCheckInterval=3,stateCheckInterval=60,inFlightDelay=600,waitTimeSeconds=20,waitTokenError=30,transferPoolSize=3,downloadMode="data+telemetry",filetype=[".fastq",".fq",".fastq.gz",".fq.gz"],signing=!0,DEFAULTS={local:local,url:url,gqlUrl:gqlUrl,user_agent:user_agent,region:region,sessionGrace:sessionGrace,uploadTimeout:uploadTimeout,downloadTimeout:downloadTimeout,fileCheckInterval:fileCheckInterval,downloadCheckInterval:downloadCheckInterval,stateCheckInterval:stateCheckInterval,inFlightDelay:inFlightDelay,waitTimeSeconds:waitTimeSeconds,waitTokenError:waitTokenError,transferPoolSize:transferPoolSize,downloadMode:downloadMode,filetype:filetype,signing:signing};class REST{constructor(e){this.options=lodash.assign({agent_version:utils.version,local:local,url:url,user_agent:user_agent,signing:signing},e),this.log=this.options.log}async list(e){try{const s=await utils.get(e,this.options),i=e.match(/^[a-z_]+/i)[0];return Promise.resolve(s[`${i}s`])}catch(t){return this.log.error(`list error ${String(t)}`),Promise.reject(t)}}async read(e,t){try{const i=await utils.get(`${e}/${t}`,this.options);return Promise.resolve(i)}catch(s){return this.log.error("read",s),Promise.reject(s)}}async user(e){let t;if(this.options.local)t={accounts:[{id_user_account:"none",number:"NONE",name:"None"}]};else try{t=await utils.get("user",this.options)}catch(s){return e?e(s):Promise.reject(s)}return e?e(null,t):Promise.resolve(t)}async status(){try{const t=await utils.get("status",this.options);return Promise.resolve(t)}catch(e){return Promise.reject(e)}}async instanceToken(e,t){try{const i=await utils.post("token",lodash.merge(t,{id_workflow_instance:e}),lodash.assign({},this.options,{legacy_form:!0}));return Promise.resolve(i)}catch(s){return Promise.reject(s)}}async installToken(e,t){try{const i=await utils.post("token/install",{id_workflow:e},lodash.assign({},this.options,{legacy_form:!0}));return t?t(null,i):Promise.resolve(i)}catch(s){return t?t(s):Promise.reject(s)}}async attributes(e){try{const s=await this.list("attribute");return e?e(null,s):Promise.resolve(s)}catch(t){return e?e(t):Promise.reject(t)}}async workflows(e){try{const s=await this.list("workflow");return e?e(null,s):Promise.resolve(s)}catch(t){return e?e(t):Promise.reject(t)}}async amiImages(e){if(this.options.local){const t=new Error("amiImages unsupported in local mode");return e?e(t):Promise.reject(t)}try{const s=this.list("ami_image");return e?e(null,s):Promise.resolve(s)}catch(t){return e?e(t):Promise.reject(t)}}async amiImage(e,t,s){let i,o,r,n;if(e&&t&&s instanceof Function?(i=e,o=t,r=s,n="update"):e&&t instanceof Object&&!(t instanceof Function)?(i=e,o=t,n="update"):e instanceof Object&&t instanceof Function?(o=e,r=t,n="create"):e instanceof Object&&!t?(o=e,n="create"):(n="read",i=e,r=t instanceof Function?t:null),this.options.local){const e=new Error("ami_image unsupported in local mode");return r?r(e):Promise.reject(e)}if("update"===n)try{const e=await utils.put("ami_image",i,o,this.options);return r?r(null,e):Promise.resolve(e)}catch(a){return r?r(a):Promise.reject(a)}if("create"===n)try{const e=await utils.post("ami_image",o,this.options);return r?r(null,e):Promise.resolve(e)}catch(a){return r?r(a):Promise.reject(a)}if(!i){const e=new Error("no id_ami_image specified");return r?r(e):Promise.reject(e)}try{const e=await this.read("ami_image",i);return r?r(null,e):Promise.resolve(e)}catch(a){return r?r(a):Promise.reject(a)}}async workflow(e,t,s){let i,o,r,n;if(e&&t&&s instanceof Function?(i=e,o=t,r=s,n="update"):e&&t instanceof Object&&!(t instanceof Function)?(i=e,o=t,n="update"):e instanceof Object&&t instanceof Function?(o=e,r=t,n="create"):e instanceof Object&&!t?(o=e,n="create"):(n="read",i=e,r=t instanceof Function?t:null),"update"===n)try{const e=await utils.put("workflow",i,o,this.options);return r?r(null,e):Promise.resolve(e)}catch(u){return r?r(u):Promise.reject(u)}if("create"===n)try{const e=await utils.post("workflow",o,this.options);return r?r(null,e):Promise.resolve(e)}catch(u){return r?r(u):Promise.reject(u)}if(!i){const e=new Error("no workflow id specified");return r?r(e):Promise.reject(e)}const a={};try{const e=await this.read("workflow",i);if(e.error)throw new Error(e.error);lodash.merge(a,e)}catch(u){return this.log.error(`${i}: error fetching workflow ${String(u)}`),r?r(u):Promise.reject(u)}lodash.merge(a,{params:{}});try{const e=await utils.get(`workflow/config/${i}`,this.options);if(e.error)throw new Error(e.error);lodash.merge(a,e)}catch(u){return this.log.error(`${i}: error fetching workflow config ${String(u)}`),r?r(u):Promise.reject(u)}const l=lodash.filter(a.params,{widget:"ajax_dropdown"}),c=[...l.map((e,t)=>{const s=l[t];return new Promise(async(e,t)=>{const i=s.values.source.replace("{{EPI2ME_HOST}}","").replace(/&?apikey=\{\{EPI2ME_API_KEY\}\}/,"");try{const o=(await utils.get(i,this.options))[s.values.data_root];return o&&(s.values=o.map(e=>({label:e[s.values.items.label_key],value:e[s.values.items.value_key]}))),e()}catch(u){return this.log.error(`failed to fetch ${i}`),t(u)}})})];try{return await Promise.all(c),r?r(null,a):Promise.resolve(a)}catch(u){return this.log.error(`${i}: error fetching config and parameters ${String(u)}`),r?r(u):Promise.reject(u)}}async startWorkflow(e,t){return utils.post("workflow_instance",e,lodash.assign({},this.options,{legacy_form:!0}),t)}stopWorkflow(e,t){return utils.put("workflow_instance/stop",e,null,lodash.assign({},this.options,{legacy_form:!0}),t)}async workflowInstances(e,t){let s,i;if(!e||e instanceof Function||void 0!==t?(s=e,i=t):i=e,i&&i.run_id)try{const e=(await utils.get(`workflow_instance/wi?show=all&columns[0][name]=run_id;columns[0][searchable]=true;columns[0][search][regex]=true;columns[0][search][value]=${i.run_id};`,this.options)).data.map(e=>({id_workflow_instance:e.id_ins,id_workflow:e.id_flo,run_id:e.run_id,description:e.desc,rev:e.rev}));return s?s(null,e):Promise.resolve(e)}catch(o){return s?s(o):Promise.reject(o)}try{const e=await this.list("workflow_instance");return s?s(null,e):Promise.resolve(e)}catch(o){return s?s(o):Promise.reject(o)}}async workflowInstance(e,t){try{const i=await this.read("workflow_instance",e);return t?t(null,i):Promise.resolve(i)}catch(s){return t?t(s):Promise.reject(s)}}workflowConfig(e,t){return utils.get(`workflow/config/${e}`,this.options,t)}async register(e,t,s){let i,o;t&&t instanceof Function?o=t:(i=t,o=s);try{const t=await utils.put("reg",e,{description:i||`${os__default.userInfo().username}@${os__default.hostname()}`},lodash.assign({},this.options,{signing:!1}));return o?o(null,t):Promise.resolve(t)}catch(r){return o?o(r):Promise.reject(r)}}async datasets(e,t){let s,i;!e||e instanceof Function||void 0!==t?(s=e,i=t):i=e,i||(i={}),i.show||(i.show="mine");try{const e=await this.list(`dataset?show=${i.show}`);return s?s(null,e):Promise.resolve(e)}catch(o){return s?s(o):Promise.reject(o)}}async dataset(e,t){if(!this.options.local)try{const i=await this.read("dataset",e);return t?t(null,i):Promise.resolve(i)}catch(s){return t?t(s):Promise.reject(s)}try{const i=(await this.datasets()).find(t=>t.id_dataset===e);return t?t(null,i):Promise.resolve(i)}catch(s){return t?t(s):Promise.reject(s)}}async fetchContent(e,t){const s=lodash.assign({},this.options,{skip_url_mangle:!0,headers:{"Content-Type":""}});try{const o=await utils.get(e,s);return t?t(null,o):Promise.resolve(o)}catch(i){return t?t(i):Promise.reject(i)}}}class REST_FS extends REST{async workflows(e){if(!this.options.local)return super.workflows(e);const t=path.join(this.options.url,"workflows");let s;try{return s=(await fs__default.readdir(t)).filter(e=>fs__default.statSync(path.join(t,e)).isDirectory()).map(e=>path.join(t,e,"workflow.json")).map(e=>fs__default.readJsonSync(e)),e?e(null,s):Promise.resolve(s)}catch(i){return this.log.warn(i),e?e(void 0):Promise.reject(void 0)}}async workflow(e,t,s){if(!this.options.local||!e||"object"===typeof e||s)return super.workflow(e,t,s);const i=path.join(this.options.url,"workflows"),o=path.join(i,e,"workflow.json");try{const e=await fs__default.readJson(o);return s?s(null,e):Promise.resolve(e)}catch(r){return s?s(r):Promise.reject(r)}}async workflowInstances(e,t){if(!this.options.local)return super.workflowInstances(e,t);let s,i;if(!e||e instanceof Function||void 0!==t?(s=e,i=t):i=e,i){const e=new Error("querying of local instances unsupported in local mode");return s?s(e):Promise.reject(e)}const o=path.join(this.options.url,"instances");try{let e=await fs__default.readdir(o);return e=(e=e.filter(e=>fs__default.statSync(path.join(o,e)).isDirectory())).map(e=>{const t=path.join(o,e,"workflow.json");let s;try{s=fs__default.readJsonSync(t)}catch(i){s={id_workflow:"-",description:"-",rev:"0.0"}}return s.id_workflow_instance=e,s.filename=t,s}),s?s(null,e):Promise.resolve(e)}catch(r){return s?s(r):Promise.reject(r)}}async datasets(e,t){if(!this.options.local)return super.datasets(e,t);let s,i;if(!e||e instanceof Function||void 0!==t?(s=e,i=t):i=e,i||(i={}),i.show||(i.show="mine"),"mine"!==i.show)return s(new Error("querying of local datasets unsupported in local mode"));const o=path.join(this.options.url,"datasets");try{let e=await fs__default.readdir(o);e=e.filter(e=>fs__default.statSync(path.join(o,e)).isDirectory());let t=0;return e=e.sort().map(e=>({is_reference_dataset:!0,summary:null,dataset_status:{status_label:"Active",status_value:"active"},size:0,prefix:e,id_workflow_instance:null,id_account:null,is_consented_human:null,data_fields:null,component_id:null,uuid:e,is_shared:!1,id_dataset:t+=1,id_user:null,last_modified:null,created:null,name:e,source:e,attributes:null})),s?s(null,e):Promise.resolve(e)}catch(r){return this.log.warn(r),s?s(null,[]):Promise.resolve([])}}async bundleWorkflow(e,t,s){return utils.pipe(`workflow/bundle/${e}.tar.gz`,t,this.options,s)}}async function bytes(e){return fs__default.stat(e).then(e=>({type:"bytes",bytes:e.size}))}function fastq(e){return new Promise(async(t,s)=>{let i,o=1,r={size:0};try{r=await fs__default.stat(e)}catch(n){return void s(n)}fs__default.createReadStream(e).on("data",e=>{i=-1,o-=1;do{i=e.indexOf(10,i+1),o+=1}while(-1!==i)}).on("end",()=>t({type:"fastq",bytes:r.size,reads:Math.floor(o/4)})).on("error",s)})}function fasta(e){return new Promise(async(t,s)=>{let i,o=1,r={size:0};try{r=await fs__default.stat(e)}catch(n){s(n)}fs__default.createReadStream(e).on("data",e=>{i=-1,o-=1;do{i=e.indexOf(62,i+1),o+=1}while(-1!==i)}).on("end",()=>t({type:"fasta",bytes:r.size,sequences:Math.floor((1+o)/2)})).on("error",s)})}const mapping={fastq:fastq,fasta:fasta,default:bytes};function filestats(e){if("string"!==typeof e&&!(e instanceof String))return Promise.resolve({});let t=path.extname(e).toLowerCase().replace(/^[.]/,"");return"fq"===t?t="fastq":"fa"===t&&(t="fasta"),mapping[t]||(t="default"),mapping[t](e)}const linesPerRead=4;function splitter(e,t){const{maxChunkBytes:s,maxChunkReads:i}=lodash.merge({},t);return new Promise(async(t,o)=>{if(!s&&!i)return void t({source:e,split:!1,chunks:[e]});const r=await fs__default.stat(e);if(s&&r.size<s)return void t({source:e,split:!1,chunks:[e]});let n,a=1,l=0,c=0,u=1;const h=[];readline.createInterface({input:fs__default.createReadStream(e)}).on("close",async()=>{n&&(await Promise.all(n.mywriters),n.close()),t({source:e,split:!0,chunks:h})}).on("line",async t=>{const o=!(c%linesPerRead);(!n||o&&(s&&l>s||i&&u>=i))&&(n=(()=>{const t=path.dirname(e),s=path.basename(e),i=s.match(/^[^.]+/)[0],o=s.replace(i,""),r=`${i}_${a}${o}`;a+=1,l=0,u=0,c=0,h.push(path.join(t,r));const n=fs__default.createWriteStream(path.join(t,r));return n.mywriters=[],n})());const r=new Promise(e=>{n.write(`${t}\n`,e)});n.mywriters.push(r),l+=t.length,c+=1,o&&(u+=1)}).on("error",o)})}const niceSize=(e,t)=>{const s=["","K","M","G","T","P","E","Z"];let i=t||0,o=e||0;return o>1e3?(o/=1e3,(i+=1)>=s.length?"???":niceSize(o,i)):0===i?`${o}${s[i]}`:`${o.toFixed(1)}${s[i]}`};class EPI2ME{constructor(e){let t;if((t="string"===typeof e||"object"===typeof e&&e.constructor===String?JSON.parse(e):e||{}).log){if(!lodash.every([t.log.info,t.log.warn,t.log.error,t.log.debug,t.log.json],lodash.isFunction))throw new Error("expected log object to have error, debug, info, warn and json methods");this.log=t.log}else this.log={info:e=>{console.info(`[${(new Date).toISOString()}] INFO: ${e}`)},debug:e=>{console.debug(`[${(new Date).toISOString()}] DEBUG: ${e}`)},warn:e=>{console.warn(`[${(new Date).toISOString()}] WARN: ${e}`)},error:e=>{console.error(`[${(new Date).toISOString()}] ERROR: ${e}`)},json:e=>{console.log(JSON.stringify(e))}};this.stopped=!0,this.states={upload:{filesCount:0,success:{files:0,bytes:0,reads:0},types:{},niceTypes:"",progress:{bytes:0,total:0}},download:{progress:{},success:{files:0,reads:0,bytes:0},fail:0,types:{},niceTypes:""},warnings:[]},this.config={options:lodash.defaults(t,DEFAULTS),instance:{id_workflow_instance:t.id_workflow_instance,inputQueueName:null,outputQueueName:null,outputQueueURL:null,discoverQueueCache:{},bucket:null,bucketFolder:null,remote_addr:null,chain:null,key_id:null}},this.config.instance.awssettings={region:this.config.options.region},this.REST=new REST(lodash.merge({},{log:this.log},this.config.options)),this.timers={downloadCheckInterval:null,stateCheckInterval:null,fileCheckInterval:null,transferTimeouts:{},visibilityIntervals:{},summaryTelemetryInterval:null}}async stopEverything(){this.stopped=!0,this.log.debug("stopping watchers"),["downloadCheckInterval","stateCheckInterval","fileCheckInterval","summaryTelemetryInterval"].forEach(e=>{this.timers[e]&&(this.log.debug(`clearing ${e} interval`),clearInterval(this.timers[e]),this.timers[e]=null)}),Object.keys(this.timers.transferTimeouts).forEach(e=>{this.log.debug(`clearing transferTimeout for ${e}`),clearTimeout(this.timers.transferTimeouts[e]),delete this.timers.transferTimeouts[e]}),Object.keys(this.timers.visibilityIntervals).forEach(e=>{this.log.debug(`clearing visibilityInterval for ${e}`),clearInterval(this.timers.visibilityIntervals[e]),delete this.timers.visibilityIntervals[e]}),this.downloadWorkerPool&&(this.log.debug("clearing downloadWorkerPool"),await Promise$1.all(Object.values(this.downloadWorkerPool)),this.downloadWorkerPool=null);const{id_workflow_instance:e}=this.config.instance;if(e){try{await this.REST.stopWorkflow(e)}catch(t){return this.log.error(`Error stopping instance: ${String(t)}`),Promise$1.reject(t)}this.log.info(`workflow instance ${e} stopped`)}return Promise$1.resolve()}async session(e,t){let s=!1;if(e&&e.length&&(s=!0),!s){if(this.sessioning)return Promise$1.resolve();if(this.states.sts_expiration&&this.states.sts_expiration>Date.now())return Promise$1.resolve();this.sessioning=!0}let i=null;try{await this.fetchInstanceToken(e,t)}catch(o){i=o,this.log.error(`session error ${String(i)}`)}finally{s||(this.sessioning=!1)}return i?Promise$1.reject(i):Promise$1.resolve()}async fetchInstanceToken(e,t){if(!this.config.instance.id_workflow_instance)return Promise$1.reject(new Error("must specify id_workflow_instance"));this.log.debug("new instance token needed");try{const i=await this.REST.instanceToken(this.config.instance.id_workflow_instance,t);this.log.debug(`allocated new instance token expiring at ${i.expiration}`),this.states.sts_expiration=new Date(i.expiration).getTime()-60*this.config.options.sessionGrace,this.config.options.proxy&&AWS.config.update({httpOptions:{agent:proxy(this.config.options.proxy,!0)}}),AWS.config.update(this.config.instance.awssettings),AWS.config.update(i),e&&e.forEach(e=>{try{e.config.update(i)}catch(t){this.log.warn(`failed to update config on ${String(e)}: ${String(t)}`)}})}catch(s){this.log.warn(`failed to fetch instance token: ${String(s)}`)}return Promise$1.resolve()}async sessionedS3(e){return await this.session(null,e),new AWS.S3({useAccelerateEndpoint:"on"===this.config.options.awsAcceleration})}async sessionedSQS(e){return await this.session(null,e),new AWS.SQS}reportProgress(){const{upload:e,download:t}=this.states;this.log.json({progress:{download:t,upload:e}})}storeState(e,t,s,i){const o=i||{};this.states[e]||(this.states[e]={}),this.states[e][t]||(this.states[e][t]={}),"incr"===s?Object.keys(o).forEach(s=>{this.states[e][t][s]=this.states[e][t][s]?this.states[e][t][s]+parseInt(o[s],10):parseInt(o[s],10)}):Object.keys(o).forEach(s=>{this.states[e][t][s]=this.states[e][t][s]?this.states[e][t][s]-parseInt(o[s],10):-parseInt(o[s],10)});try{this.states[e].success.niceReads=niceSize(this.states[e].success.reads)}catch(n){this.states[e].success.niceReads=0}try{this.states[e].progress.niceSize=niceSize(this.states[e].success.bytes+this.states[e].progress.bytes||0)}catch(n){this.states[e].progress.niceSize=0}try{this.states[e].success.niceSize=niceSize(this.states[e].success.bytes)}catch(n){this.states[e].success.niceSize=0}this.states[e].niceTypes=Object.keys(this.states[e].types||{}).sort().map(t=>`${this.states[e].types[t]} ${t}`).join(", ");const r=Date.now();(!this.stateReportTime||r-this.stateReportTime>2e3)&&(this.stateReportTime=r,this.reportProgress())}uploadState(e,t,s){return this.storeState("upload",e,t,s)}downloadState(e,t,s){return this.storeState("download",e,t,s)}async deleteMessage(e){try{const s=await this.discoverQueue(this.config.instance.outputQueueName);return(await this.sessionedSQS()).deleteMessage({QueueUrl:s,ReceiptHandle:e.ReceiptHandle}).promise()}catch(t){return this.log.error(`deleteMessage exception: ${String(t)}`),this.states.download.failure||(this.states.download.failure={}),this.states.download.failure[t]=this.states.download.failure[t]?this.states.download.failure[t]+1:1,Promise$1.reject(t)}}async discoverQueue(e){if(this.config.instance.discoverQueueCache[e])return Promise$1.resolve(this.config.instance.discoverQueueCache[e]);let t;this.log.debug(`discovering queue for ${e}`);try{const i=await this.sessionedSQS();t=await i.getQueueUrl({QueueName:e}).promise()}catch(s){return this.log.error(`Error: failed to find queue for ${e}: ${String(s)}`),Promise$1.reject(s)}return this.log.debug(`found queue ${t.QueueUrl}`),this.config.instance.discoverQueueCache[e]=t.QueueUrl,Promise$1.resolve(t.QueueUrl)}async queueLength(e){if(!e)return Promise$1.reject(new Error("no queueURL specified"));const t=e.match(/([\w\-_]+)$/)[0];this.log.debug(`querying queue length of ${t}`);try{const t=await this.sessionedSQS(),i=await t.getQueueAttributes({QueueUrl:e,AttributeNames:["ApproximateNumberOfMessages"]}).promise();if(i&&i.Attributes&&"ApproximateNumberOfMessages"in i.Attributes){let e=i.Attributes.ApproximateNumberOfMessages;return e=parseInt(e,10)||0,Promise$1.resolve(e)}return Promise$1.reject(new Error("unexpected response"))}catch(s){return this.log.error(`error in getQueueAttributes ${String(s)}`),Promise$1.reject(s)}}url(){return this.config.options.url}apikey(){return this.config.options.apikey}attr(e,t){if(!(e in this.config.options))throw new Error(`config object does not contain property ${e}`);return t?(this.config.options[e]=t,this):this.config.options[e]}stats(e){return this.states[e]}}EPI2ME.version=utils.version,EPI2ME.REST=REST,EPI2ME.utils=utils;class db{constructor(e,t,s){const i=lodash.merge({},t);this.options=i,this.log=s;const{idWorkflowInstance:o}=i;s.debug(`setting up ${e}/db.sqlite for ${o}`),this.db=fs.mkdirp(e).then(()=>(this.log.debug(`opening ${e}/db.sqlite`),sqlite.open(path.join(e,"db.sqlite"),{Promise:Promise}).then(async t=>{this.log.debug(`opened ${e}/db.sqlite`);try{return await Promise.all([t.run("CREATE TABLE IF NOT EXISTS meta (version CHAR(12) DEFAULT '' NOT NULL, idWorkflowInstance INTEGER UNSIGNED, inputFolder CHAR(255) default '')").then(()=>{t.run("INSERT INTO meta (version, idWorkflowInstance, inputFolder) VALUES(?, ?, ?)",pkg.version,o,i.inputFolder)}),t.run("CREATE TABLE IF NOT EXISTS uploads (filename CHAR(255) DEFAULT '' NOT NULL PRIMARY KEY)"),t.run("CREATE TABLE IF NOT EXISTS skips (filename CHAR(255) DEFAULT '' NOT NULL PRIMARY KEY)")]),Promise.resolve(t)}catch(s){return this.log.error(s),Promise.reject(s)}}))).catch(e=>{throw this.log.error(e),e})}async uploadFile(e){const t=await this.db,s=e.replace(new RegExp(`^${this.options.inputFolder}`),"");return this.log.info(`MARKING ${s} as uploaded`),t.run("INSERT INTO uploads VALUES(?)",s)}async skipFile(e){const t=await this.db,s=e.replace(new RegExp(`^${this.options.inputFolder}`),"");return t.run("INSERT INTO skips VALUES(?)",s)}async seenUpload(e){const t=await this.db,s=e.replace(new RegExp(`^${this.options.inputFolder}`),"");return Promise.all([t.get("SELECT * FROM uploads u WHERE u.filename=? LIMIT 1",s),t.get("SELECT * FROM skips s WHERE s.filename=? LIMIT 1",s)]).then(e=>lodash.remove(e,void 0).length)}}class Profile{constructor(e,t){this.prefsFile=e||Profile.profilePath(),this.profileCache={},this.defaultEndpoint=process.env.METRICHOR||DEFAULTS.endpoint||DEFAULTS.url,this.raiseExceptions=t;try{const e=fs__default.readJSONSync(this.prefsFile);this.profileCache=lodash.merge(e.profiles,{}),e.endpoint&&(this.defaultEndpoint=e.endpoint)}catch(s){if(this.raiseExceptions)throw s}}static profilePath(){return path.join(os.homedir(),".epi2me.json")}profile(e,t){if(e&&t){const i=lodash.merge(this.profileCache,{[e]:t});try{fs__default.writeJSONSync(this.prefsFile,{profiles:i})}catch(s){if(this.raiseExceptions)throw s}this.profileCache=i}return e?lodash.merge({endpoint:this.defaultEndpoint},this.profileCache[e]):{}}profiles(){return Object.keys(this.profileCache||{})}}const rootDir=()=>{const e=process.env.APPDATA||("darwin"===process.platform?path.join(os.homedir(),"Library/Application Support"):os.homedir());return process.env.EPI2ME_HOME||path.join(e,"linux"===process.platform?".epi2me":"EPI2ME")};class EPI2ME_FS extends EPI2ME{constructor(e){super(e),this.REST=new REST_FS(lodash.merge({},{log:this.log},this.config.options))}async autoStart(e,t){let s;this.stopped=!1;try{s=await this.REST.startWorkflow(e)}catch(i){const e=`Failed to start workflow: ${String(i)}`;return this.log.warn(e),t?t(e):Promise$1.reject(i)}return this.config.workflow=JSON.parse(JSON.stringify(e)),this.log.info(`instance ${JSON.stringify(s)}`),this.log.info(`workflow config ${JSON.stringify(this.config.workflow)}`),this.autoConfigure(s,t)}async autoJoin(e,t){let s;this.stopped=!1,this.config.instance.id_workflow_instance=e;try{s=await this.REST.workflowInstance(e)}catch(i){const e=`Failed to join workflow instance: ${String(i)}`;return this.log.warn(e),t?t(e):Promise$1.reject(i)}return"stopped"===s.state?(this.log.warn(`workflow ${e} is already stopped`),t?t("could not join workflow"):Promise$1.reject(new Error("could not join workflow"))):(this.config.workflow=this.config.workflow||{},this.log.debug(`instance ${JSON.stringify(s)}`),this.log.debug(`workflow config ${JSON.stringify(this.config.workflow)}`),this.autoConfigure(s,t))}async autoConfigure(e,t){if(["id_workflow_instance","id_workflow","remote_addr","key_id","bucket","user_defined","start_date"].forEach(t=>{this.config.instance[t]=e[t]}),this.config.instance.inputQueueName=e.inputqueue,this.config.instance.outputQueueName=e.outputqueue,this.config.instance.awssettings.region=e.region||this.config.options.region,this.config.instance.bucketFolder=`${e.outputqueue}/${e.id_user}/${e.id_workflow_instance}`,this.config.instance.summaryTelemetry=e.telemetry,e.chain)if("object"===typeof e.chain)this.config.instance.chain=e.chain;else try{this.config.instance.chain=JSON.parse(e.chain)}catch(a){throw new Error(`exception parsing chain JSON ${String(a)}`)}if(!this.config.options.inputFolder)throw new Error("must set inputFolder");if(!this.config.options.outputFolder)throw new Error("must set outputFolder");if(!this.config.instance.bucketFolder)throw new Error("bucketFolder must be set");if(!this.config.instance.inputQueueName)throw new Error("inputQueueName must be set");if(!this.config.instance.outputQueueName)throw new Error("outputQueueName must be set");fs__default.mkdirpSync(this.config.options.outputFolder);const s=path.join(rootDir(),"instances"),i=path.join(s,this.config.instance.id_workflow_instance);this.db=new db(i,{idWorkflowInstance:this.config.instance.id_workflow_instance,inputFolder:this.config.options.inputFolder},this.log);const o=this.config.instance.id_workflow_instance?`telemetry-${this.config.instance.id_workflow_instance}.log`:"telemetry.log",r=path.join(this.config.options.outputFolder,"epi2me-logs"),n=path.join(r,o);return fs__default.mkdirp(r,e=>{if(e&&!String(e).match(/EEXIST/))this.log.error(`error opening telemetry log stream: mkdirpException:${String(e)}`);else try{this.telemetryLogStream=fs__default.createWriteStream(n,{flags:"a"}),this.log.info(`logging telemetry to ${n}`)}catch(t){this.log.error(`error opening telemetry log stream: ${String(t)}`)}}),t&&t(null,this.config.instance),this.timers.summaryTelemetryInterval=setInterval(()=>{this.fetchTelemetry()},1e4*this.config.options.downloadCheckInterval),this.timers.downloadCheckInterval=setInterval(()=>{this.checkForDownloads()},1e3*this.config.options.downloadCheckInterval),this.timers.stateCheckInterval=setInterval(async()=>{try{const s=await this.REST.workflowInstance(this.config.instance.id_workflow_instance);if("stopped"===s.state){this.log.warn(`instance was stopped remotely at ${s.stop_date}. shutting down the workflow.`);try{const t=await this.stopEverything();"function"===typeof t.config.options.remoteShutdownCb&&t.config.options.remoteShutdownCb(`instance was stopped remotely at ${s.stop_date}`)}catch(e){this.log.error(`Error whilst stopping: ${String(e)}`)}}}catch(t){this.log.warn(`failed to check instance state: ${t&&t.error?t.error:t}`)}},1e3*this.config.options.stateCheckInterval),await this.session(),this.reportProgress(),this.loadUploadFiles(),this.timers.fileCheckInterval=setInterval(this.loadUploadFiles.bind(this),1e3*this.config.options.fileCheckInterval),Promise$1.resolve(e)}async checkForDownloads(){if(this.checkForDownloadsRunning)return Promise$1.resolve();this.checkForDownloadsRunning=!0,this.log.debug("checkForDownloads checking for downloads");try{const t=await this.discoverQueue(this.config.instance.outputQueueName),s=await this.queueLength(t);s?(this.log.debug(`downloads available: ${s}`),await this.downloadAvailable()):this.log.debug("no downloads available")}catch(e){this.log.warn(`checkForDownloads error ${String(e)}`),this.states.download.failure||(this.states.download.failure={}),this.states.download.failure[e]=this.states.download.failure[e]?this.states.download.failure[e]+1:1}return this.checkForDownloadsRunning=!1,Promise$1.resolve()}async downloadAvailable(){const e=Object.keys(this.downloadWorkerPool||{}).length;if(e>=this.config.options.transferPoolSize)return this.log.debug(`${e} downloads already queued`),Promise$1.resolve();let t;try{const i=await this.discoverQueue(this.config.instance.outputQueueName);this.log.debug("fetching messages");const o=await this.sessionedSQS();t=await o.receiveMessage({AttributeNames:["All"],QueueUrl:i,VisibilityTimeout:this.config.options.inFlightDelay,MaxNumberOfMessages:this.config.options.transferPoolSize-e,WaitTimeSeconds:this.config.options.waitTimeSeconds}).promise()}catch(s){return this.log.error(`receiveMessage exception: ${String(s)}`),this.states.download.failure[s]=this.states.download.failure[s]?this.states.download.failure[s]+1:1,Promise$1.reject(s)}return this.receiveMessages(t)}async loadUploadFiles(){if(this.dirScanInProgress)return Promise$1.resolve();this.dirScanInProgress=!0,this.log.debug("upload: started directory scan");try{const t=e=>this.db.seenUpload(e),s=await utils.loadInputFiles(this.config.options,this.log,t);let i=0;const o=()=>new Promise$1(async e=>{if(this.stopped)return s.length=0,this.log.debug("upload: skipping, stopped"),void e();if(i>this.config.options.transferPoolSize)return void setTimeout(e,1e3);const t=s.splice(0,this.config.options.transferPoolSize-i);i+=t.length;try{await this.enqueueUploadFiles(t)}catch(o){this.log.error(`upload: exception in enqueueUploadFiles: ${String(o)}`)}i-=t.length,e()});for(;s.length;)await o()}catch(e){this.log.error(`upload: exception in loadInputFiles: ${String(e)}`)}return this.dirScanInProgress=!1,this.log.debug("upload: finished directory scan"),Promise$1.resolve()}async enqueueUploadFiles(e){let t=0,s=0,i={};if(!lodash.isArray(e)||!e.length)return Promise$1.resolve();if(this.log.info(`enqueueUploadFiles ${e.length} files: ${e.map(e=>e.path)}.`),"workflow"in this.config)if("workflow_attributes"in this.config.workflow)i=this.config.workflow.workflow_attributes;else if("attributes"in this.config.workflow){let{attributes:e}=this.config.workflow.attributes;if(e||(e={}),"epi2me:max_size"in e&&(i.max_size=parseInt(e["epi2me:max_size"],10)),"epi2me:max_files"in e&&(i.max_files=parseInt(e["epi2me:max_files"],10)),"epi2me:category"in e){e["epi2me:category"].includes("storage")&&(i.requires_storage=!0)}}if(this.log.info(`enqueueUploadFiles settings ${JSON.stringify(i)}`),"requires_storage"in i&&i.requires_storage&&!("storage_account"in this.config.workflow)){const e={msg:"ERROR: Workflow requires storage enabled. Please provide a valid storage account [ --storage ].",type:"WARNING_STORAGE_ENABLED"};return this.log.error(e.msg),this.states.warnings.push(e),Promise$1.resolve()}if("max_size"in i&&(s=parseInt(i.max_size,10),this.log.info(`enqueueUploadFiles restricting file size to ${s}`)),"max_files"in i&&(t=parseInt(i.max_files,10),this.log.info(`enqueueUploadFiles restricting file count to ${t}`),e.length>t)){const s={msg:`ERROR: ${e.length} files found. Workflow can only accept ${t}. Please move the extra files away.`,type:"WARNING_FILE_TOO_MANY"};return this.log.error(s.msg),this.states.warnings.push(s),Promise$1.resolve()}this.states.upload.filesCount+=e.length;const o=e.map(async e=>{const i=e;if(this.log.info(`INPUTBATCHQUEUE ${i.path}, ${s}, ${i.size}`),t&&this.states.upload.filesCount>t){const e=`Maximum ${t} file(s) already uploaded. Marking ${i.relative} as skipped.`,s={msg:e,type:"WARNING_FILE_TOO_MANY"};this.log.error(e),this.states.warnings.push(s),this.states.upload.filesCount-=1,i.skip="SKIP_TOO_MANY"}else if(0===i.size){const e=`The file "${i.relative}" is empty. It will be skipped.`,t={msg:e,type:"WARNING_FILE_EMPTY"};i.skip="SKIP_EMPTY",this.states.upload.filesCount-=1,this.log.error(e),this.states.warnings.push(t)}else{if(i.path&&i.path.match(/\.(?:fastq|fq)$/)&&s&&i.size>s){const e=`${i.relative} is too big and is going to be split`;this.log.warn(e);const t={msg:e,type:"WARNING_FILE_SPLIT"};this.states.warnings.push(t);const s=await splitter(i.path,{maxChunkReads:4e3}),o=utils.getFileID();let r=1;const n=s.chunks.map(async e=>{const t={name:path.parse(e).base,path:e,relative:e.replace(this.config.options.inputFolder,""),id:`${o}_${r}`};return r+=1,filestats(e).then(e=>(t.stats=e,t)).catch(t=>{this.error(`failed to stat chunk ${e}: ${String(t)}`)})}).map(e=>e.then(e=>(this.log.info(`chunk ${JSON.stringify(e)}`),this.uploadJob(e).then(()=>fs__default.unlink(e.path)).catch(t=>{this.error(`failed to unlink chunk ${e.path}: ${String(t)}`)}))));return Promise$1.all(n).then(()=>this.db.uploadFile(i.path))}if(s&&i.size>s){const e=`The file "${i.relative}" is bigger than the maximum size limit (${niceSize(s)}B). It will be skipped.`,t={msg:e,type:"WARNING_FILE_TOO_BIG"};i.skip="SKIP_TOO_BIG",this.states.upload.filesCount-=1,this.log.error(e),this.states.warnings.push(t)}else try{i.stats=await filestats(i.path)}catch(o){this.error(`failed to stat ${i.path}: ${String(o)}`)}}return this.uploadJob(i)});try{return await Promise$1.all(o),this.log.info(`upload: inputBatchQueue (${o.length} jobs) complete`),this.loadUploadFiles()}catch(r){return this.log.error(`upload: enqueueUploadFiles exception ${String(r)}`),Promise$1.reject(r)}}async uploadJob(e){if("skip"in e)return this.db.skipFile(e.path);let t,s;try{this.log.info(`upload: ${e.id} starting`),t=await this.uploadHandler(e),this.log.info(`upload: ${t.id} uploaded and notified`)}catch(i){s=i,this.log.error(`upload: ${e.id} done, but failed: ${String(s)}`)}if(t||(t={}),s)this.log.error(`uploadJob ${s}`),this.states.upload.failure||(this.states.upload.failure={}),this.states.upload.failure[s]=this.states.upload.failure[s]?this.states.upload.failure[s]+1:1;else if(this.uploadState("success","incr",lodash.merge({files:1},t.stats)),t.name){const e=path.extname(t.name);this.uploadState("types","incr",{[e]:1})}return Promise$1.resolve()}async receiveMessages(e){return e&&e.Messages&&e.Messages.length?(this.downloadWorkerPool||(this.downloadWorkerPool={}),e.Messages.forEach(e=>{this.downloadWorkerPool[e.MessageId]=1;const t=setTimeout(()=>{throw this.log.error(`this.downloadWorkerPool timeoutHandle. Clearing queue slot for message: ${e.MessageId}`),new Error("download timed out")},1e3*(60+this.config.options.downloadTimeout));this.processMessage(e).catch(e=>{this.log.error(`processMessage ${String(e)}`)}).finally(()=>{clearTimeout(t),delete this.downloadWorkerPool[e.MessageId]})}),this.log.info(`downloader queued ${e.Messages.length} messages for processing`),Promise$1.resolve()):(this.log.info("complete (empty)"),Promise$1.resolve())}async processMessage(e){let t,s;if(!e)return this.log.debug("download.processMessage: empty message"),Promise$1.resolve();"Attributes"in e&&"ApproximateReceiveCount"in e.Attributes&&this.log.debug(`download.processMessage: ${e.MessageId} / ${e.Attributes.ApproximateReceiveCount}`);try{t=JSON.parse(e.Body)}catch(n){this.log.error(`error parsing JSON message.Body from message: ${JSON.stringify(e)} ${String(n)}`);try{await this.deleteMessage(e)}catch(a){this.log.error(`Exception deleting message: ${String(a)}`)}return Promise$1.resolve()}if(t.telemetry){const{telemetry:s}=t;if(s.tm_path)try{this.log.debug(`download.processMessage: ${e.MessageId} fetching telemetry`);const i=await this.sessionedS3(),o=await i.getObject({Bucket:t.bucket,Key:s.tm_path}).promise();this.log.info(`download.processMessage: ${e.MessageId} fetched telemetry`),s.batch=o.Body.toString("utf-8").split("\n").filter(e=>e&&e.length>0).map(e=>{try{return JSON.parse(e)}catch(a){return this.log.error(`Telemetry Batch JSON Parse error: ${String(a)}`),e}})}catch(l){this.log.error(`Could not fetch telemetry JSON: ${String(l)}`)}try{this.telemetryLogStream.write(JSON.stringify(s)+os.EOL)}catch(c){this.log.error(`error writing telemetry: ${c}`)}this.config.options.telemetryCb&&this.config.options.telemetryCb(s)}if(!t.path)return this.log.warn("nothing to download"),Promise$1.resolve();const i=t.path.match(/[\w\W]*\/([\w\W]*?)$/),o=i?i[1]:"";if(s=this.config.options.outputFolder,t.telemetry&&t.telemetry.hints&&t.telemetry.hints.folder){this.log.debug(`using folder hint ${t.telemetry.hints.folder}`);const e=t.telemetry.hints.folder.split("/").map(e=>e.toUpperCase());s=path.join.apply(null,[s,...e])}fs__default.mkdirpSync(s);const r=path.join(s,o);if("data+telemetry"===this.config.options.downloadMode){const s=[""];let i=this.config&&this.config.workflow&&this.config.workflow.settings&&this.config.workflow.settings.output_format?this.config.workflow.settings.output_format:[];("string"===typeof i||i instanceof String)&&(i=i.trim().split(/[\s,]+/));try{s.push(...i)}catch(a){this.log.error(`Failed to work out workflow file suffixes: ${String(a)}`)}try{const i=s.map(s=>{const i=t.path+s,o=r+s;return this.log.debug(`download.processMessage: ${e.MessageId} downloading ${i} to ${o}`),new Promise$1(async(r,n)=>{try{await this.initiateDownloadStream({bucket:t.bucket,path:i},e,o)}catch(a){this.log.error(`Caught exception waiting for initiateDownloadStream: ${String(a)}`),s&&n(a)}r()})});await Promise$1.all(i)}catch(a){this.log.error(`Exception fetching file batch: ${String(a)}`)}try{const e=!(!t.telemetry||!t.telemetry.json)&&t.telemetry.json.exit_status;e&&this.config.options.dataCb&&this.config.options.dataCb(r,e)}catch(l){this.log.warn(`failed to fire data callback: ${l}`)}}else{const e=t.telemetry.batch_summary&&t.telemetry.batch_summary.reads_num?t.telemetry.batch_summary.reads_num:1;this.downloadState("success","incr",{files:1,reads:e})}try{await this.deleteMessage(e)}catch(a){this.log.error(`Exception deleting message: ${String(a)}`)}return Promise$1.resolve()}async initiateDownloadStream(e,t,s){return new Promise$1(async(i,o)=>{let r,n,a;try{r=await this.sessionedS3()}catch(u){o(u)}const l=t=>{if(this.log.error(`Error during stream of bucket=${e.bucket} path=${e.path} to file=${s} ${String(t)}`),clearTimeout(this.timers.transferTimeouts[s]),delete this.timers.transferTimeouts[s],!n.networkStreamError)try{n.networkStreamError=1,n.close(),fs__default.remove(s).then(()=>{this.log.warn(`removed failed download ${s}`)}).catch(e=>{this.log.warn(`failed to remove ${s}. unlinkException: ${String(e)}`)}),a.destroy&&(this.log.error(`destroying read stream for ${s}`),a.destroy())}catch(u){this.log.error(`error handling stream error: ${String(u)}`)}};try{const t={Bucket:e.bucket,Key:e.path};n=fs__default.createWriteStream(s);const i=r.getObject(t);i.on("httpHeaders",(e,t)=>{this.downloadState("progress","incr",{total:parseInt(t["content-length"],10)})}),a=i.createReadStream()}catch(h){return this.log.error(`getObject/createReadStream exception: ${String(h)}`),void o(h)}a.on("error",l),n.on("finish",async()=>{if(!n.networkStreamError){this.log.debug(`downloaded ${s}`);try{const t=path.extname(s),i=await filestats(s);this.downloadState("success","incr",lodash.merge({files:1},i)),this.downloadState("types","incr",{[t]:1}),this.downloadState("progress","decr",{total:i.bytes,bytes:i.bytes})}catch(e){this.log.warn(`failed to stat ${s}: ${String(e)}`)}this.reportProgress()}}),n.on("close",o=>{this.log.debug(`closing writeStream ${s}`),o&&this.log.error(`error closing write stream ${o}`),clearInterval(this.timers.visibilityIntervals[s]),delete this.timers.visibilityIntervals[s],clearTimeout(this.timers.transferTimeouts[s]),delete this.timers.transferTimeouts[s],setTimeout(this.checkForDownloads.bind(this)),this.log.info(`download.initiateDownloadStream: ${t.MessageId} downloaded ${e.path} to ${s}`),i()}),n.on("error",l);const c=()=>{l(new Error("transfer timed out"))};this.timers.transferTimeouts[s]=setTimeout(c,1e3*this.config.options.downloadTimeout);this.timers.visibilityIntervals[s]=setInterval(async()=>{const e=this.config.instance.outputQueueURL,i=t.ReceiptHandle;this.log.debug({message_id:t.MessageId},"updateVisibility");try{await this.sqs.changeMessageVisibility({QueueUrl:e,ReceiptHandle:i,VisibilityTimeout:this.config.options.inFlightDelay}).promise()}catch(o){this.log.error({message_id:t.MessageId,queue:e,error:o},"Error setting visibility"),clearInterval(this.timers.visibilityIntervals[s])}},900*this.config.options.inFlightDelay),a.on("data",e=>{clearTimeout(this.timers.transferTimeouts[s]),this.timers.transferTimeouts[s]=setTimeout(c,1e3*this.config.options.downloadTimeout),this.downloadState("progress","incr",{bytes:e.length})}).pipe(n)})}async uploadHandler(e){const t=await this.sessionedS3();let s;const i=e.relative.replace(/^[\\\/]+/,"").replace(/\\/g,"/").replace(/\//g,"_"),o=[this.config.instance.bucketFolder,"component-0",i,i].join("/").replace(/\/+/g,"/");let r;return new Promise$1((i,n)=>{const a=()=>{s&&!s.closed&&s.close(),n(new Error(`${e.name} timed out`))};r=setTimeout(a,1e3*(this.config.options.uploadTimeout+5));try{s=fs__default.createReadStream(e.path)}catch(l){return clearTimeout(r),void n(l)}s.on("error",e=>{s.close();let t="error in upload readstream";e&&e.message&&(t+=`: ${e.message}`),clearTimeout(r),n(new Error(t))}),s.on("open",()=>{const l={Bucket:this.config.instance.bucket,Key:o,Body:s};this.config.instance.key_id&&(l.SSEKMSKeyId=this.config.instance.key_id,l.ServerSideEncryption="aws:kms"),e.size&&(l["Content-Length"]=e.size),this.uploadState("progress","incr",{total:e.size});let c=0;const u=t.upload(l,{partSize:10485760,queueSize:1});u.on("httpUploadProgress",async e=>{this.uploadState("progress","incr",{bytes:e.loaded-c}),c=e.loaded,clearTimeout(r),r=setTimeout(a,1e3*(this.config.options.uploadTimeout+5));try{await this.session([u.service])}catch(t){this.log.warn(`Error refreshing token: ${String(t)}`)}}),u.promise().then(()=>{this.log.info(`${e.id} S3 upload complete`),s.close(),clearTimeout(r),this.uploadComplete(o,e).then(()=>{i(e)}).catch(e=>{n(e)}).finally(()=>{this.uploadState("progress","decr",{total:e.size,bytes:e.size})})}).catch(t=>{this.log.warn(`${e.id} uploadStreamError ${t}`),n(t)})})})}async uploadComplete(e,t){this.log.info(`${t.id} uploaded to S3: ${e}`);const s={bucket:this.config.instance.bucket,outputQueue:this.config.instance.outputQueueName,remote_addr:this.config.instance.remote_addr,user_defined:this.config.instance.user_defined||null,apikey:this.config.options.apikey,id_workflow_instance:this.config.instance.id_workflow_instance,id_master:this.config.instance.id_workflow,utc:(new Date).toISOString(),path:e,prefix:e.substring(0,e.lastIndexOf("/"))};if(this.config.instance.chain)try{s.components=JSON.parse(JSON.stringify(this.config.instance.chain.components)),s.targetComponentId=this.config.instance.chain.targetComponentId}catch(i){return this.log.error(`${t.id} exception parsing components JSON ${String(i)}`),Promise$1.reject(i)}if(this.config.instance.key_id&&(s.key_id=this.config.instance.key_id),this.config.options.agent_address)try{s.agent_address=JSON.parse(this.config.options.agent_address)}catch(o){this.log.error(`${t.id} Could not parse agent_address ${String(o)}`)}s.components&&Object.keys(s.components).forEach(e=>{"uploadMessageQueue"===s.components[e].inputQueueName&&(s.components[e].inputQueueName=this.uploadMessageQueue),"downloadMessageQueue"===s.components[e].inputQueueName&&(s.components[e].inputQueueName=this.downloadMessageQueue)});try{const e=await this.discoverQueue(this.config.instance.inputQueueName),i=await this.sessionedSQS();this.log.info(`${t.id} sending SQS message to input queue`),await i.sendMessage({QueueUrl:e,MessageBody:JSON.stringify(s)}).promise()}catch(r){return this.log.error(`${t.id} exception sending SQS message: ${String(r)}`),Promise$1.reject(r)}return this.log.info(`${t.id} SQS message sent. Mark as uploaded`),this.db.uploadFile(t.path)}async fetchTelemetry(){if(!this.config||!this.config.instance||!this.config.instance.summaryTelemetry)return Promise$1.resolve();const e=path.join(rootDir(),"instances"),t=path.join(e,this.config.instance.id_workflow_instance),s=[];Object.keys(this.config.instance.summaryTelemetry).forEach(e=>{const i=this.config.instance.summaryTelemetry[e]||{},o=i[Object.keys(i)[0]];if(!o)return;const r=path.join(t,`${e}.json`);s.push(this.REST.fetchContent(o).then(e=>{fs__default.writeJSONSync(r,e),this.log.debug(`fetched telemetry summary ${r}`)}).catch(e=>{this.log.debug(`Error fetching telemetry: ${String(e)}`)}))});let i=0;try{await Promise$1.all(s)}catch(o){i+=1}return i&&this.log.warn("summary telemetry incomplete"),Promise$1.resolve()}}EPI2ME_FS.version=utils.version,EPI2ME_FS.REST=REST_FS,EPI2ME_FS.utils=utils,EPI2ME_FS.EPI2ME_HOME=rootDir(),EPI2ME_FS.Profile=Profile,module.exports=EPI2ME_FS;
